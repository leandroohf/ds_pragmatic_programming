{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "**WIP** It is still a messy\n",
    "\n",
    "**TODO**\n",
    "\n",
    "1. ~Vectorization operations and loss functions~\n",
    "\n",
    "    * ~create ones and zeros and get shape~\n",
    "    * ~matrix n vector mutliplications~\n",
    "    * ~reshape matrix~\n",
    "    * ~How to encode one-hot~\n",
    "\n",
    "1. ~Ho to define models~ \n",
    "\n",
    "    * ~Helow word simple regression~\n",
    "    * ~define models with sequence~\n",
    "    \n",
    "1. Callbacks \n",
    "    * How to stop when reach specific acc or perfromance \n",
    "    * How to save best model during \n",
    "    * How to save models archicterue, parameters, optmizer parameters (everything) (see my code in dialects)\n",
    "    * How to load saved models \n",
    "    \n",
    "1. Code for main loss functions and when to use\n",
    "\n",
    "    * binnary cross entropy\n",
    "    * categorical cross entropy\n",
    "    * sparse categorical cross entropy\n",
    "    * mae\n",
    "    * mse \n",
    "\n",
    "1. Learning curves \n",
    "    * Code to plot learning curves in notebook using matplotlib\n",
    "\n",
    "1. Code to use Genrators\n",
    "    * Split folders in train and val so Generators can read \n",
    "    * ImageDataGenerator with data augmentation\n",
    "    * Use ImageDataGenerator to save aigmented data\n",
    "\n",
    "refs:\n",
    "* https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:21:22.344578Z",
     "start_time": "2019-05-16T01:21:11.912985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor basic operations \n",
    "\n",
    "https://www.tensorflow.org/guide/tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector add: [4 6]\n",
      "sum all elemenst: 6\n",
      "mymat: [[ 7]\n",
      " [11]]\n",
      "batch_imgs: (10, 299, 299, 3)\n",
      "\n",
      "loss: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"vector add: {tf.add([1, 2], [3, 4])}\")\n",
    "print(f\"sum all elemenst: {tf.reduce_sum([1, 2, 3])}\")\n",
    "\n",
    "mymat = tf.Variable([[7],[11]], tf.int16)\n",
    "print(f\"mymat: {mymat.value()}\")\n",
    "\n",
    "# batch x height x width x color\n",
    "batch_imgs = tf.zeros([10, 299, 299, 3])  \n",
    "print(f\"batch_imgs: {batch_imgs.shape}\")\n",
    "\n",
    "print()\n",
    "\n",
    "y_hat = tf.constant(36)            # Define y_hat constant. Set to 36.\n",
    "y = tf.constant(39)   \n",
    "\n",
    "loss = tf.Variable((y - y_hat)**2)\n",
    "\n",
    "print(f\"loss: {loss.value()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, stack, transpose and numpy compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy compatibility\n",
      "Tensor convert to numpy\n",
      "tensor: [[10. 10. 10.]\n",
      " [10. 10. 10.]\n",
      " [10. 10. 10.]]\n",
      "And Tensors converts to numpy\n",
      " add scalr to tensor and get a numpy array:  [[11. 11. 11.]\n",
      " [11. 11. 11.]\n",
      " [11. 11. 11.]]\n",
      "\n",
      "The .numpy() method explicitly converts a Tensor to a numpy array\n",
      "[[10. 10. 10.]\n",
      " [10. 10. 10.]\n",
      " [10. 10. 10.]]\n",
      "\n",
      "tensor ones: <tf.Variable 'Variable:0' shape=(3, 3) dtype=float64, numpy=\n",
      "array([[1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.]])>\n",
      "\n",
      "Reshaping\n",
      "x shape: (3, 3)\n",
      "x: [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "y shape: (2, 6)\n",
      "x: [[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n",
      "\n",
      "Transpose\n",
      "y shape: (2, 6)\n",
      "y.T: (6, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"numpy compatibility\")\n",
    "\n",
    "ndarray = np.ones([3, 3])\n",
    "\n",
    "print(\"Tensor convert to numpy\")\n",
    "tensor = tf.multiply(ndarray, 10)\n",
    "print(f\"tensor: {tensor}\")\n",
    "\n",
    "print(\"And Tensors converts to numpy\")\n",
    "print(f\" add scalr to tensor and get a numpy array:  {np.add(tensor, 1)}\")\n",
    "\n",
    "print()\n",
    "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
    "print(tensor.numpy())\n",
    "\n",
    "print()\n",
    "print(f\"tensor ones: {tf.Variable(np.ones([3, 3]))}\")\n",
    "\n",
    "print()\n",
    "print(\"Reshaping\")\n",
    "\n",
    "x = tf.reshape(np.array([1,2,3,4,5,6,7,8,9]), [3, 3])\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"x: {x.numpy()}\")\n",
    "\n",
    "# -1 means infere this dimension\n",
    "y = tf.reshape(np.array([1,2,3,4,5,6,7,8,9,10,11,12]), [2, -1])\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"x: {y.numpy()}\")\n",
    "\n",
    "print()\n",
    "print(\"Transpose\")\n",
    "# Transpose\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y.T: {tf.transpose(y).shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack or concatamete\n",
      "v shape: (3, 2)\n",
      "v: [[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "v shape: (2, 3)\n",
      "v: [[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Stack or concatamete\")\n",
    "x = tf.constant([1, 4])\n",
    "y = tf.constant([2, 5])\n",
    "z = tf.constant([3, 6])\n",
    "\n",
    "v = tf.stack([x, y, z])\n",
    "\n",
    "print(f\"v shape: {v.shape}\")\n",
    "print(f\"v: {v.numpy()}\")\n",
    "\n",
    "\n",
    "v = tf.stack([x, y, z], axis=1)\n",
    "\n",
    "print(f\"v shape: {v.shape}\")\n",
    "print(f\"v: {v.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randon tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 4, 8, 8], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-2.2767038 ,  2.2165687 , -0.4222828 , -0.41158316], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.dtypes.float32, seed=None, name=None)\n",
    "x = tf.random.uniform([5],0,10, dtype = tf.int32, seed = 0)\n",
    "x.numpy()\n",
    "\n",
    "# tf.random.normal(shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None, name=None)\n",
    "x = tf.random.normal([4], 0, 1, tf.float32)\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello word (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb336fb8940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking\n",
      "x: 3.0; expected: 5.0; prediction: [[5.0003324]]\n",
      "unseen data\n",
      "x: 10.0; expected: 19.0; prediction: [[18.97712]]\n"
     ]
    }
   ],
   "source": [
    "# the data\n",
    "# rule that We need to find is y = 2x - 1\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0,1.0,3.0,5.0,7.0], dtype=float)\n",
    "\n",
    "# Define a model with one neuron\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
    "\n",
    "# define loss func and optimizer \n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# train or learn the rule\n",
    "model.fit(xs, ys, epochs=500, verbose=0)\n",
    "\n",
    "print(\"Checking\")\n",
    "x = 3.0\n",
    "print(f\"x: {x}; expected: {2.0*x-1}; prediction: {model.predict([x])}\")\n",
    "\n",
    "print(\"unseen data\")\n",
    "x = 10.00\n",
    "print(f\"x: {x}; expected: {2.0*x-1}; prediction: {model.predict([x])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:16:21.750111Z",
     "start_time": "2019-05-14T23:16:21.746973Z"
    }
   },
   "source": [
    "## Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* One hot encoding\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "\n",
    "<img src=\"images/onehot.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (6, 4)\n",
      "label(4): 2; one-hot encode: [0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "test_labels = tf.constant([1,2, 3, 0, 2, 1])\n",
    "\n",
    "y = to_categorical(test_labels)\n",
    "print(f\"output shape: {y.shape}\")\n",
    "\n",
    "k = 4\n",
    "print(f\"label({k}): {test_labels[k]}; one-hot encode: {y[k,:]}\")\n",
    "y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [0, 1, 2]\n",
    "depth = 3\n",
    "\n",
    "# ?tf.one_hot\n",
    "tf.one_hot(indices, depth)  # output: [3 x 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loss and metrics \n",
    "\n",
    "ref: https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### MAE and MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_true = np.array([1  ,0, 1, 0])\n",
    "y_pred = np.array([0.5,0, 1, 0])\n",
    "\n",
    "mse = tf.keras.losses.MSE(y_true, y_pred).numpy()\n",
    "mae = tf.keras.losses.MAE(y_true, y_pred).numpy()\n",
    "\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"mae: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Cross entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "```python\n",
    "\n",
    "# binary \n",
    "tf.keras.losses.binary_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, label_smoothing=0\n",
    ")\n",
    "```\n",
    "\n",
    "* binary cross entropy\n",
    "\n",
    "$\n",
    "- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log (\\hat{y}^{(i)}) + (1-y^{(i)})\\log (1-\\hat{y}^{(i)} \\large ) \\small\\tag{1}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.constant([1,0, 1, 0])\n",
    "\n",
    "# y_pred must be float because it means probabilities\n",
    "y_pred = tf.constant([0.3,0, 0.9, 0])\n",
    "\n",
    "_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "print(f\"loss: {_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "```python\n",
    "# categorical crossentropy\n",
    "tf.keras.losses.categorical_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, label_smoothing=0\n",
    ")\n",
    "\n",
    "# sparse\n",
    "tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, axis=-1\n",
    ")\n",
    "\n",
    "# Use the loss function (approx. 1 line)\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## TODO needs to be reviewed \n",
    "y_true = to_categorical(tf.constant([1,0, 1, 0]))\n",
    "\n",
    "# y_pred must be float because it means probabilities\n",
    "y_pred = to_categorical(tf.constant([0.3,0, 0.9, 0]))\n",
    "\n",
    "_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "print(f\"loss: {_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## TODO needs to be reviewed \n",
    "y_true = tf.constant([1,2, 1, 0])\n",
    "\n",
    "\n",
    "\n",
    "# y_pred must be float because it means probabilities\n",
    "y_pred = tf.constant([ [0.0,0.9, 0.1],\n",
    "                       [0.0,0.3, 0.7],\n",
    "                       [0.5,0.9, 0.5],\n",
    "                       [0.7,0.1, 0.2]\n",
    "                     ])\n",
    "\n",
    "y_pred.shape\n",
    "y_pred\n",
    "\n",
    "_loss = tf.keras.losses.categorical_crossentropy(y_true,y_pred)\n",
    "\n",
    "print(f\"loss: {_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "* sigmoid_cross_entropy_with_logits\n",
    "\n",
    "```python\n",
    "\n",
    "# Use the loss function (approx. 1 line)\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
    "\n",
    "```\n",
    "\n",
    "$\n",
    "- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small\\tag{2}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers: \n",
    "\n",
    " Understanding and Debbuging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "        \n",
    "$\n",
    "W_o = \\frac{(W_i + P_w  - K_w)}{S_w} + 1\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "* $W_i$: input image width\n",
    "* $P_w$: padding, the total number of elements to add in top and botton of the input image\n",
    "* $K_w$: kernell width\n",
    "* $S_w$: stride width. number of elements to hop \n",
    "\n",
    "\n",
    "> PS: To check just make $K_w = 1$, $P_w$ and  $S_w = 1$, we expecting same image size.\n",
    "\n",
    "\n",
    "The eqation is similar for the output height $H_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1],\n",
       "        [ 1,  2],\n",
       "        [ 2,  3],\n",
       "        [ 3,  4],\n",
       "        [ 4,  5],\n",
       "        [ 5,  6],\n",
       "        [ 6,  7],\n",
       "        [ 7,  8],\n",
       "        [ 8,  9],\n",
       "        [ 9, 10]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([range(10),range(10)])\n",
    "X[1,:] = X[1,:] + 1 \n",
    "X = np.reshape(X,(2,10,1))\n",
    "X.shape\n",
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 10, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output size: (8.0, 1)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 8, 1)              4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 8, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1],\n",
       "        [ 1,  2],\n",
       "        [ 2,  3],\n",
       "        [ 3,  4],\n",
       "        [ 4,  5],\n",
       "        [ 5,  6],\n",
       "        [ 6,  7],\n",
       "        [ 7,  8],\n",
       "        [ 8,  9],\n",
       "        [ 9, 10]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Conv1d\")\n",
    "\n",
    "_kernel_init_coefs = np.array([-1, 0, 1])\n",
    "_kernel_init_coefs = tf.initializers.constant(_kernel_init_coefs)\n",
    "\n",
    "# Rememember the output of a conv layer change the output size\n",
    "# conv 1d: Output size = 1 + (in size + padding size - kernel size)/(stride size)\n",
    "\n",
    "# H W C \n",
    "X.shape\n",
    "\n",
    "in_size = X.shape[1]\n",
    "pad_size = 0\n",
    "kernel_size = 3\n",
    "stride_size = 1\n",
    "w_size = ( in_size + pad_size - kernel_size)/(stride_size) + 1  # w direction\n",
    "h_size = 1\n",
    "out_size = (w_size, h_size)\n",
    "\n",
    "print(f\"Expected output size: {out_size}\")\n",
    "model3 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(1, kernel_size=3, kernel_initializer= _kernel_init_coefs, input_shape=(10,1) )\n",
    "    ])\n",
    "\n",
    "model3.summary()\n",
    "y3 = model3.predict(X)\n",
    "y3.shape\n",
    "\n",
    "X.T\n",
    "\n",
    "# Expected all 2 values becasue: \n",
    "# 0*-1 + 1*0 + 2*1 = 2\n",
    "# 1*-1 + 2*0 + 3*1 = 2\n",
    "# 2*-1 + 3*0 + 4*1 = 2\n",
    "# ...\n",
    "# 7*-1 + 8*0 + 9*1 = 2\n",
    "y3.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GlobalAveragePooling1D and MaxPool1D\n",
    "\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D\n",
    "\n",
    "```python\n",
    "tf.keras.layers.MaxPool1D(\n",
    "    pool_size=2, strides=None, padding='valid', data_format='channels_last',\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\n",
    "# filters is the number of neurons\n",
    "tf.keras.layers.Conv1D(\n",
    "    filters, kernel_size, strides=1, padding='valid', data_format='channels_last',\n",
    "    dilation_rate=1, activation=None, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalAveragePooling1D\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling1d (Gl (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[4.5, 5.5]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MaxPool1D\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "max_pooling1d (MaxPooling1D) (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 5, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  2.],\n",
       "        [ 3.,  4.],\n",
       "        [ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"GlobalAveragePooling1D\")\n",
    "model1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.GlobalAveragePooling1D(input_shape=(10,1) )\n",
    "    ])\n",
    "\n",
    "model1.summary()\n",
    "y1 = model1.predict(X)\n",
    "y1.shape\n",
    "y1.T\n",
    "\n",
    "print()\n",
    "print(\"MaxPool1D\")\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.MaxPool1D(pool_size=2, input_shape=(10,1) )\n",
    "    ])\n",
    "\n",
    "model2.summary()\n",
    "y2 = model2.predict(X)\n",
    "y2.shape\n",
    "\n",
    "y2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing with Lambda Layer\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 10, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3340ff5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[4.5, 5.5]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_layer(x):\n",
    "    \n",
    "    tensor = tf.reduce_mean(x,axis=1)\n",
    "        \n",
    "    return tensor\n",
    "\n",
    "\n",
    "print(f\"Implementing with Lambda Layer\")\n",
    "model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(10,1), name=\"input_layer\"),\n",
    "        tf.keras.layers.Lambda( lambda x: custom_layer(x)  )\n",
    "    ])\n",
    "\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "X.shape\n",
    "#X\n",
    "\n",
    "y2 = model2.predict(X)\n",
    "y2.shape\n",
    "y2.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v shape: (2, 4, 3)\n",
      "Implementing with Lambda Layer\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3340ad1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.25     , 2.       , 3.25     , 0.4330127, 0.       , 1.47902  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.25     , 2.       , 3.25     , 0.4330127, 0.       , 1.47902  ],\n",
       "       [2.25     , 2.25     , 3.5      , 0.4330127, 0.4330127, 1.5      ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stat_pooling_layer(x):\n",
    "    \n",
    "    tensor_std = tf.math.reduce_std(x,axis=1)\n",
    "    tensor_mean = tf.reduce_mean(x,axis=1)\n",
    "    \n",
    "    tensor = tf.concat([tensor_mean, tensor_std], axis=1)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# batch size = 2, max seq = 4, mfcc dim = 3\n",
    "X1 = tf.constant([[[1, 2,3], [1,2, 4], [1, 2, 5], [2, 2, 1]]])\n",
    "X2 = tf.constant([[[2, 3,4], [2,2, 4], [2, 2, 5], [3, 2, 1]]])\n",
    "\n",
    "V = tf.concat([X1,X2], axis=0)\n",
    "\n",
    "print(f\"v shape: {V.shape}\")\n",
    "\n",
    "print(f\"Implementing with Lambda Layer\")\n",
    "model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(4,3), name=\"input_layer\"),\n",
    "        #tf.keras.layers.Lambda( lambda x: tf.math.reduce_std(x,axis=1) )\n",
    "        tf.keras.layers.Lambda( lambda x: stat_pooling_layer(x)  )\n",
    "    ])\n",
    "\n",
    "model2.summary()\n",
    "y2 = model2.predict(V)\n",
    "y2.shape\n",
    "\n",
    "\n",
    "# For computing the std for double checking\n",
    "# mfccs = [3,4,5,1]\n",
    "# np.mean(mfccs)\n",
    "# np.std(mfccs)\n",
    "# Expected first sequence: Mean = [ 1.25, 2.0, 3.25 ] std = [.433, 0.0, 1.4790 ]\n",
    "# y2[0,:,:] =  [ 3 means, 3 stds]\n",
    "y2[0,:] \n",
    "\n",
    "print()\n",
    "y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input are the values of x over a batch: $B = {x_1, x_2,..., x_i,..., x_m}$\n",
    "    * where $m$ is the batch size\n",
    "* Output: $y_i = BN_{\\gamma,\\beta}(x_i)$\n",
    "* Learning parameters: $\\gamma$ and $\\beta$\n",
    "* Normalization:\n",
    "\n",
    "$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i \\\\\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\\\\n",
    "z_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} \\\\\n",
    "y_i = BN_{\\gamma,\\beta}(x_i) \\equiv \\gamma z_i + \\beta\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.00372284, 4.84130788, 1.56712089, 3.46161308, 4.38194576,\n",
       "       4.47303332, 0.42522106, 0.19527392, 0.8491521 , 4.39071252])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 2.86; std: 1.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.64399874,  1.11517084, -0.72667862,  0.33904225,  0.85676291,\n",
       "        0.90800298, -1.36903888, -1.49839253, -1.13056223,  0.86169453])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.64399874,  1.11517084, -0.72667862,  0.33904225,  0.85676291,\n",
       "        0.90800298, -1.36903888, -1.49839253, -1.13056223,  0.86169453])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 0.00; std: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Batch\n",
    "X = np.random.uniform(0,5.0,size=(10))\n",
    "X\n",
    "\n",
    "print(f\"shape: {X.shape}; mean: {X.mean():.2f}; std: {X.std():.2f}\")\n",
    "\n",
    "# Coputing using tensorflow layer\n",
    "gamma = 1.0\n",
    "beta = 0.0\n",
    "epsilon = 0.0\n",
    "\n",
    "# because we did not train the layer, we are passing the mean and the variance of the batch\n",
    "Y = tf.nn.batch_normalization(X,\n",
    "                    mean = X.mean(axis=0),        # batch mean\n",
    "                    variance = X.var(axis=0),     # batch var\n",
    "                    offset = beta,scale = gamma,  # batch beta and gamma See equations  \n",
    "                    variance_epsilon = epsilon)   # batch epsilon See equations\n",
    "\n",
    "Y.numpy()\n",
    "\n",
    "# comparing with numpy\n",
    "\n",
    "Z = (X - X.mean(axis=0))/np.sqrt(X.var(axis=0) + epsilon)\n",
    "Y = gamma * Z + beta\n",
    "Y\n",
    "\n",
    "# Expectd zero mean and unit variance\n",
    "print(f\"shape: {Y.shape}; mean: {Y.mean():.2f}; std: {Y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###  Analyze the output of the Layers\n",
    "\n",
    "How to debbug and inspect the output of each layer of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 8, 1)              4         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 4, 1)              0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Layer: 0\n",
      "WARNING:tensorflow:7 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3340adc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "output shape: (2, 8, 1)\n",
      "[[[ 3.  6.]\n",
      "  [ 6.  9.]\n",
      "  [ 9. 12.]\n",
      "  [12. 15.]\n",
      "  [15. 18.]\n",
      "  [18. 21.]\n",
      "  [21. 24.]\n",
      "  [24. 27.]]]\n",
      "Layer: 1\n",
      "output shape: (2, 4, 1)\n",
      "[[[ 6.  9.]\n",
      "  [12. 15.]\n",
      "  [18. 21.]\n",
      "  [24. 27.]]]\n",
      "Layer: 2\n",
      "output shape: (2, 1)\n",
      "[[15. 18.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "_kernel_init_coefs = np.array([1, 1, 1])\n",
    "_kernel_init_coefs = tf.initializers.constant(_kernel_init_coefs)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(1, kernel_size=3, kernel_initializer= _kernel_init_coefs, input_shape=(10,1) ),\n",
    "        tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "        tf.keras.layers.GlobalAveragePooling1D()\n",
    "    ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "\n",
    "for l in range(3):\n",
    "    \n",
    "    print(f\"Layer: {l}\")\n",
    "    \n",
    "    f1 = activation_model.predict(X)[l]\n",
    "    print(f\"output shape: {f1.shape}\")\n",
    "    print(f1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Notes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "ref:\n",
    "* https://datascience.stackexchange.com/questions/29719/how-to-set-batch-size-steps-per-epoch-and-validation-steps\n",
    "\n",
    "\n",
    "* train_size  = batch_size x steps_per_epochs (very important to match to utilize all samples in train dataset)\n",
    "* val_size  = batch_size x validation_steps\n",
    "\n",
    "**Recipe**\n",
    "\n",
    "1. Overfit a toy train dataset: (check code)\n",
    "    * Consider overfit subset of the classes or labels for multiclass and multilabel classifiers\n",
    "    * Consider overfitt all classses or labels\n",
    "    \n",
    "1. Reduce bias\n",
    "    * Set a gioal: train mse < 0.01 or train acc > 0.90\n",
    "    * Consider train longer as possible\n",
    "    * Change architecture parameters\n",
    "\n",
    "1. Tune learning rate (keep constant #epochs)\n",
    "    * Run the experiment with small number of epochs first to have an idea of the ranges\n",
    "        * Try to determine the larges lr posiible (The loss will not explode) \n",
    "    * Run with small number of lr candidates and epochs\n",
    "    * Leave running during the night (train longer)\n",
    "    \n",
    "1. Reduce variance (keep constant #epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _load_check_point(keras_model, chk_path, _epoch):\n",
    "    \n",
    "    checkpoint_path2 = \"checkpoints/20200514_0/keras_xv_model-0100.ckpt\"\n",
    "    checkpoint_dir2 = os.path.dirname(checkpoint_path2)\n",
    "\n",
    "    print(f\"load checkoint in folder: {checkpoint_dir2}\")\n",
    "    \n",
    "    (loss, mae) = keras_model.evaluate(x = X_train_mfcc2,y = xv_train, verbose=0)\n",
    "\n",
    "    print (\"Before load loss = \" + str(loss))\n",
    "\n",
    "     keras_model.load_weights(checkpoint_path2.format(epoch=100))\n",
    "\n",
    "    (loss, mae) = keras_model.evaluate(x = X_train_mfcc2,y = xv_train, verbose=0)\n",
    "\n",
    "    print (\"After load loss = \" + str(loss))\n",
    "        \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_number = '_1' + '/'\n",
    "\n",
    "checkpoint_dir = \"checkpoints/\" + \\\n",
    "            datetime.datetime.now().strftime(\"%Y%m%d\") + model_number\n",
    "\n",
    "checkpoint_path = checkpoint_dir + \"model-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir\n",
    "\n",
    "!mkdir -p {checkpoint_dir}\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='min',\n",
    "                                                 verbose=1, \n",
    "                                                 save_frequency = 10) # save every 5 epochs\n",
    "\n",
    "\n",
    "loss_name = 'MSE'\n",
    "metric_name = 'MAE'\n",
    "def build_model(max_seq_len, dimension):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(512, 5, activation='linear', input_shape=(max_seq_len, dimension)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=3),\n",
    "        tf.keras.layers.Conv1D(256, 3, strides=2 ,activation='linear'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=3),\n",
    "        tf.keras.layers.Conv1D(128, 3,strides=3, activation='linear'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(512, activation='linear',name='xvectors')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dimension = 23\n",
    "\n",
    "#special_value = 99.0\n",
    "max_seq_len = X_train_mfcc2.shape[1]\n",
    "\n",
    "model = build_model(max_seq_len,dimension)\n",
    "\n",
    "print('X_train shape:{}'.format(X_train_mfcc2.shape))\n",
    "print('X_val shape:{}'.format(X_val_mfcc2.shape))\n",
    "print('y_train shape:{}'.format(xv_train.shape))\n",
    "print('y_val shape:{}'.format(xv_val.shape))\n",
    "print('#frame trimmed: {}'.format(max_seq_len))\n",
    "print('#mfcc: {}'.format(dimension))\n",
    "print('checkpoint: {}'.format(checkpoint_path))\n",
    "#print('logs: {}'.format(log_dir))\n",
    "#print('batch size: {}'.format(BATCH_SIZE))\n",
    "print('loss: {}'.format(loss_name))\n",
    "print('metric: {}'.format(metric_name))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_adam_opt = K.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss=loss_name, optimizer=_adam_opt, metrics=[metric_name])\n",
    "\n",
    "print('lr: {}'.format(model.optimizer.lr.numpy()))\n",
    "\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "history = keras_model.fit(X_train_mfcc2, xv_train,\n",
    "                 epochs=EPOCHS,\n",
    "                 steps_per_epoch=30, \n",
    "                 #batch_size=BATCH_SIZE,\n",
    "                 validation_data = (X_val_mfcc2, xv_val),\n",
    "                 #validation_steps=VALIDATION_STEPS,\n",
    "                 callbacks = [cp_callback]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Learning curve diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check learning curves\n",
    "\n",
    "metric_df = pd.DataFrame(keras_model.history.history)\n",
    "metric_df.columns\n",
    "\n",
    "print(f\"min loss value: {metric_df[['loss']].min()}\")\n",
    "\n",
    "metric_df[['loss', 'val_loss']].plot()\n",
    "plt.title('Loss & MAE Per EPOCH')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('loss or mae')\n",
    "#plt.ylim(0.0,1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Estimation how many epochs do you need and how long is going to take\n",
    "\n",
    "def _estimate_delta_loss(loss, start_epoch, end_epoch, n_smooth):\n",
    "    \n",
    "    # loss \n",
    "    bg = start_epoch - n_smooth\n",
    "    ed = start_epoch\n",
    "    loss_start = np.mean(loss[bg:ed]) # /6.0\n",
    "    \n",
    "    bg = end_epoch - n_smooth\n",
    "    ed = end_epoch\n",
    "    loss_end = np.mean(loss[bg:ed]) # /6.0\n",
    "    \n",
    "    delta = loss_end - loss_start\n",
    "    \n",
    "    return delta\n",
    "\n",
    "start_epoch = 200\n",
    "end_epoch = 300\n",
    "n_smooth = 6\n",
    "\n",
    "delta_loss = _estimate_delta_loss(loss0, start_epoch, end_epoch, n_smooth)\n",
    "delta_val_loss = _estimate_delta_loss(val_loss0, start_epoch, end_epoch, n_smooth)\n",
    "delta_epoch = end_epoch - start_epoch\n",
    "\n",
    "print(f\"delta loss: {delta_loss:.4f} every {delta_epoch}\")\n",
    "print(f\"delta val: {delta_val_loss:.4f} every {delta_epoch}\")\n",
    "\n",
    "\n",
    "total_time_in_hour = 3*60 + 5\n",
    "\n",
    "total_epoch = 2 * 300\n",
    "time_per_100epoch = total_time_in_hour*100/total_epoch\n",
    "\n",
    "print()\n",
    "print(f\"Simple estimation\")\n",
    "for _epoch in [200, 250, 300, 600, 1000, 1500, 2000]:\n",
    "    \n",
    "    time_est = _epoch/100 * time_per_100epoch\n",
    "    loss_est = delta_loss/delta_epoch * _epoch + loss0[start_epoch]\n",
    "    val_est = delta_val_loss/delta_epoch * _epoch + val_loss0[start_epoch]\n",
    "    \n",
    "    print(f\"epoch: {_epoch}; expected loss: {loss_est:.3f}; expected val loss: {val_est:.3f} takes: {time_est:.2f} m;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tune learning rate \n",
    "\n",
    "* lr ranges: 1e-8 - 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "start_lr = 1e-4\n",
    "number_of_lr_per_order = 15\n",
    "    \n",
    "lrs = start_lr*(10**(np.arange(EPOCHS - 1) / number_of_lr_per_order))\n",
    "\n",
    "end_lr = lrs[-1]\n",
    "\n",
    "print(f\"Learning rate search space: start: {lrs[0]}; end: {end_lr:.3f} \")\n",
    "print(f\"Searching {number_of_lr_per_order} per magnitude order\")\n",
    "\n",
    "plt.plot(lrs, 'o--')\n",
    "plt.title('Learning Rate search space');\n",
    "plt.ylabel('learning rate');\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(lrs, 'o--')\n",
    "plt.title('Learning Rate search space');\n",
    "plt.ylabel('learning rate');\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# After training\n",
    "_loss = history.history[\"loss\"]\n",
    "\n",
    "print(f\"Checking dimensions matches: lrs: {len(lrs)}; len history: {_loss}\")\n",
    "\n",
    "# x =  lr and y = loss\n",
    "plt.semilogx(lrs,_loss ,'o--' )\n",
    "#plt.axis([start_lr, end_lr, 0, 5])\n",
    "\n",
    "lrs[5:10]\n",
    "_loss[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Checking lr candidates\n",
    "\n",
    "EPOCHS = 55\n",
    "\n",
    "lr_candidates = [ 0.01, 0.006,0.001, 0.00045 ]\n",
    "\n",
    "lr_curves = list()\n",
    "for lr in lr_candidates:\n",
    "    \n",
    "    print(f\"Training with lr: {lr}\")\n",
    "    model = xv_model(max_seq_len,dimension)\n",
    "    \n",
    "    _adam_opt = K.optimizers.Adam(lr=lr)\n",
    "    model.compile(loss=loss_name, optimizer=_adam_opt, metrics=[metric_name])\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_mfcc2, xv_train,\n",
    "                 epochs=EPOCHS,\n",
    "                 steps_per_epoch=100, #EVALUATION_INTERVAL,\n",
    "                 #batch_size=BATCH_SIZE,\n",
    "                 validation_data = (X_val_mfcc2, xv_val),\n",
    "                 #validation_steps=VALIDATION_STEPS\n",
    "                )\n",
    "    \n",
    "    lr_curves.append((lr, history.history[\"loss\"], history.history[\"val_loss\"]  ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Ploting the results\n",
    "epochs = range(EPOCHS)\n",
    "\n",
    "for lr, loss, val_loss in lr_curves:\n",
    "    \n",
    "    print(f\"lr: {lr}; loss size: {len(loss)}; start: {loss[0]:.3f}; end: {loss[-1]:.3f}; epoch size: {len(epochs)}\")\n",
    "    \n",
    "    _ = plt.plot(epochs,loss,  label='lr: {}'.format(lr));\n",
    "\n",
    "\n",
    "_ = plt.title('Learning curves');\n",
    "_ = plt.ylabel('loss');\n",
    "_ = plt.xlabel('epoch');\n",
    "#_ = plt.ylim(0.0,1.5)\n",
    "_ = plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Continue train from checkpoint (transfer learning)\n",
    "\n",
    "* If you load an external model can be transfer learning tehcnique\n",
    "* Ability to change the learning rate or optmizer and start train from the last checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Review this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "\n",
    "- **Training set**: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).\n",
    "- **Test set**: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:20:05.450391Z",
     "start_time": "2019-05-15T22:20:05.405403Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    train_dataset = h5py.File('data/train_signs.h5', \"r\")\n",
    "    \n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('data/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Y_train_orig.shape\n",
    "\n",
    "type(Y_train_orig)\n",
    "\n",
    "Y_train_orig[0,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:20:06.228960Z",
     "start_time": "2019-05-15T22:20:06.076697Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(classes))\n",
    "print(classes)\n",
    "\n",
    "# See example of image\n",
    "index = 0\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* preprocessing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:48:24.233605Z",
     "start_time": "2019-05-14T23:48:24.164274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "# Flatten the training and test images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_flatten / 255.\n",
    "X_test = X_test_flatten / 255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "print(\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[1]))\n",
    "\n",
    "print()\n",
    "print('64*64*3 = 12288')\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))\n",
    "\n",
    "print()\n",
    "print (\"Y_test (1st 5) = \\n\" + str(np.squeeze(Y_test[:,0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convolutional (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:22:57.512166Z",
     "start_time": "2019-05-15T03:22:57.488726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datasets import flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "369.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

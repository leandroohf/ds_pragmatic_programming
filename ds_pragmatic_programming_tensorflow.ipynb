{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "**WIP** It is still a messy\n",
    "\n",
    "**TODO**\n",
    "\n",
    "1. ~Vectorization operations and loss functions~\n",
    "\n",
    "    * ~create ones and zeros and get shape~\n",
    "    * ~matrix n vector mutliplications~\n",
    "    * ~reshape matrix~\n",
    "    * ~How to encode one-hot~\n",
    "\n",
    "1. ~Ho to define models~ \n",
    "\n",
    "    * ~Helow word simple regression~\n",
    "    * ~define models with sequence~\n",
    "    \n",
    "1. Callbacks \n",
    "    * How to stop when reach specific acc or perfromance \n",
    "    * How to save best model during \n",
    "    * How to save models archicterue, parameters, optmizer parameters (everything) (see my code in dialects)\n",
    "    * How to load saved models \n",
    "    \n",
    "1. Code for main loss functions and when to use\n",
    "\n",
    "    * binnary cross entropy\n",
    "    * categorical cross entropy\n",
    "    * sparse categorical cross entropy\n",
    "    * mae\n",
    "    * mse \n",
    "\n",
    "1. Learning curves \n",
    "    * Code to plot learning curves in notebook using matplotlib\n",
    "\n",
    "1. Code to use Genrators\n",
    "    * Split folders in train and val so Generators can read \n",
    "    * ImageDataGenerator with data augmentation\n",
    "    * Use ImageDataGenerator to save aigmented data\n",
    "\n",
    "refs:\n",
    "* https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T01:21:22.344578Z",
     "start_time": "2019-05-16T01:21:11.912985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor basic operations \n",
    "\n",
    "https://www.tensorflow.org/guide/tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector add: [4 6]\n",
      "sum all elemenst: 6\n",
      "mymat: [[ 7]\n",
      " [11]]\n",
      "batch_imgs: (10, 299, 299, 3)\n",
      "\n",
      "loss: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"vector add: {tf.add([1, 2], [3, 4])}\")\n",
    "print(f\"sum all elemenst: {tf.reduce_sum([1, 2, 3])}\")\n",
    "\n",
    "mymat = tf.Variable([[7],[11]], tf.int16)\n",
    "print(f\"mymat: {mymat.value()}\")\n",
    "\n",
    "# batch x height x width x color\n",
    "batch_imgs = tf.zeros([10, 299, 299, 3])  \n",
    "print(f\"batch_imgs: {batch_imgs.shape}\")\n",
    "\n",
    "print()\n",
    "\n",
    "y_hat = tf.constant(36)            # Define y_hat constant. Set to 36.\n",
    "y = tf.constant(39)   \n",
    "\n",
    "loss = tf.Variable((y - y_hat)**2)\n",
    "\n",
    "print(f\"loss: {loss.value()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape, stack, transpose and numpy compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy compatibility\n",
      "Tensor convert to numpy\n",
      "tensor: [[10. 10. 10.]\n",
      " [10. 10. 10.]\n",
      " [10. 10. 10.]]\n",
      "And Tensors converts to numpy\n",
      " add scalr to tensor and get a numpy array:  [[11. 11. 11.]\n",
      " [11. 11. 11.]\n",
      " [11. 11. 11.]]\n",
      "\n",
      "The .numpy() method explicitly converts a Tensor to a numpy array\n",
      "[[10. 10. 10.]\n",
      " [10. 10. 10.]\n",
      " [10. 10. 10.]]\n",
      "\n",
      "tensor ones: <tf.Variable 'Variable:0' shape=(3, 3) dtype=float64, numpy=\n",
      "array([[1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.]])>\n",
      "\n",
      "Reshaping\n",
      "x shape: (3, 3)\n",
      "x: [[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "y shape: (2, 6)\n",
      "x: [[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]]\n",
      "\n",
      "Transpose\n",
      "y shape: (2, 6)\n",
      "y.T: (6, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"numpy compatibility\")\n",
    "\n",
    "ndarray = np.ones([3, 3])\n",
    "\n",
    "print(\"Tensor convert to numpy\")\n",
    "tensor = tf.multiply(ndarray, 10)\n",
    "print(f\"tensor: {tensor}\")\n",
    "\n",
    "print(\"And Tensors converts to numpy\")\n",
    "print(f\" add scalr to tensor and get a numpy array:  {np.add(tensor, 1)}\")\n",
    "\n",
    "print()\n",
    "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
    "print(tensor.numpy())\n",
    "\n",
    "print()\n",
    "print(f\"tensor ones: {tf.Variable(np.ones([3, 3]))}\")\n",
    "\n",
    "print()\n",
    "print(\"Reshaping\")\n",
    "\n",
    "x = tf.reshape(np.array([1,2,3,4,5,6,7,8,9]), [3, 3])\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"x: {x.numpy()}\")\n",
    "\n",
    "# -1 means infere this dimension\n",
    "y = tf.reshape(np.array([1,2,3,4,5,6,7,8,9,10,11,12]), [2, -1])\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"x: {y.numpy()}\")\n",
    "\n",
    "print()\n",
    "print(\"Transpose\")\n",
    "# Transpose\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y.T: {tf.transpose(y).shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v shape: (3, 2)\n",
      "v: [[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "v shape: (2, 3)\n",
      "v: [[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "###### print(\"Stack or concatamete\")\n",
    "x = tf.constant([1, 4])\n",
    "y = tf.constant([2, 5])\n",
    "z = tf.constant([3, 6])\n",
    "\n",
    "v = tf.stack([x, y, z])\n",
    "\n",
    "print(f\"v shape: {v.shape}\")\n",
    "print(f\"v: {v.numpy()}\")\n",
    "\n",
    "\n",
    "v = tf.stack([x, y, z], axis=1)\n",
    "\n",
    "print(f\"v shape: {v.shape}\")\n",
    "print(f\"v: {v.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 8, 5, 3], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.29522768,  0.41140285,  1.5405939 , -2.106709  ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.dtypes.float32, seed=None, name=None)\n",
    "x = tf.random.uniform([5],0,10, dtype = tf.int32, seed = 0)\n",
    "x.numpy()\n",
    "\n",
    "# tf.random.normal(shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None, name=None)\n",
    "x = tf.random.normal([4], 0, 1, tf.float32)\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello word (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e073160>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking\n",
      "x: 3.0; expected: 5.0; prediction: [[5.000212]]\n",
      "unseen data\n",
      "x: 10.0; expected: 19.0; prediction: [[18.985415]]\n"
     ]
    }
   ],
   "source": [
    "# the data\n",
    "# rule that We need to find is y = 2x - 1\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0,1.0,3.0,5.0,7.0], dtype=float)\n",
    "\n",
    "# Define a model with one neuron\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
    "\n",
    "# define loss func and optimizer \n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# train or learn the rule\n",
    "model.fit(xs, ys, epochs=500, verbose=0)\n",
    "\n",
    "print(\"Checking\")\n",
    "x = 3.0\n",
    "print(f\"x: {x}; expected: {2.0*x-1}; prediction: {model.predict([x])}\")\n",
    "\n",
    "print(\"unseen data\")\n",
    "x = 10.00\n",
    "print(f\"x: {x}; expected: {2.0*x-1}; prediction: {model.predict([x])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.keras.layers.Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:16:21.750111Z",
     "start_time": "2019-05-14T23:16:21.746973Z"
    }
   },
   "source": [
    "## Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* One hot encoding\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "\n",
    "<img src=\"images/onehot.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (6, 4)\n",
      "label(4): 2; one-hot encode: [0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "test_labels = tf.constant([1,2, 3, 0, 2, 1])\n",
    "\n",
    "y = to_categorical(test_labels)\n",
    "print(f\"output shape: {y.shape}\")\n",
    "\n",
    "k = 4\n",
    "print(f\"label({k}): {test_labels[k]}; one-hot encode: {y[k,:]}\")\n",
    "y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [0, 1, 2]\n",
    "depth = 3\n",
    "\n",
    "# ?tf.one_hot\n",
    "tf.one_hot(indices, depth)  # output: [3 x 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and metrics \n",
    "\n",
    "ref: https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE and MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.0625\n",
      "mae: 0.125\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([1  ,0, 1, 0])\n",
    "y_pred = np.array([0.5,0, 1, 0])\n",
    "\n",
    "mse = tf.keras.losses.MSE(y_true, y_pred).numpy()\n",
    "mae = tf.keras.losses.MAE(y_true, y_pred).numpy()\n",
    "\n",
    "print(f\"mse: {mse}\")\n",
    "print(f\"mae: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "\n",
    "# binary \n",
    "tf.keras.losses.binary_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, label_smoothing=0\n",
    ")\n",
    "```\n",
    "\n",
    "* binary cross entropy\n",
    "\n",
    "$\n",
    "- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log (\\hat{y}^{(i)}) + (1-y^{(i)})\\log (1-\\hat{y}^{(i)} \\large ) \\small\\tag{1}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.constant([1,0, 1, 0])\n",
    "\n",
    "# y_pred must be float because it means probabilities\n",
    "y_pred = tf.constant([0.3,0, 0.9, 0])\n",
    "\n",
    "_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "print(f\"loss: {_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "# categorical crossentropy\n",
    "tf.keras.losses.categorical_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, label_smoothing=0\n",
    ")\n",
    "\n",
    "# sparse\n",
    "tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    y_true, y_pred, from_logits=False, axis=-1\n",
    ")\n",
    "\n",
    "# Use the loss function (approx. 1 line)\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO needs to be reviewed \n",
    "y_true = to_categorical(tf.constant([1,0, 1, 0]))\n",
    "\n",
    "# y_pred must be float because it means probabilities\n",
    "y_pred = to_categorical(tf.constant([0.3,0, 0.9, 0]))\n",
    "\n",
    "_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "print(f\"loss: {_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO needs to be reviewed \n",
    "y_true = tf.constant([1,2, 1, 0])\n",
    "\n",
    "\n",
    "\n",
    "# y_pred must be float because it means probabilities\n",
    "y_pred = tf.constant([ [0.0,0.9, 0.1],\n",
    "                       [0.0,0.3, 0.7],\n",
    "                       [0.5,0.9, 0.5],\n",
    "                       [0.7,0.1, 0.2]\n",
    "                     ])\n",
    "\n",
    "y_pred.shape\n",
    "y_pred\n",
    "\n",
    "_loss = tf.keras.losses.categorical_crossentropy(y_true,y_pred)\n",
    "\n",
    "print(f\"loss: {_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* sigmoid_cross_entropy_with_logits\n",
    "\n",
    "```python\n",
    "\n",
    "# Use the loss function (approx. 1 line)\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n",
    "\n",
    "```\n",
    "\n",
    "$\n",
    "- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small\\tag{2}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers: \n",
    "\n",
    " Understanding and Debbuging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "  \n",
    "$\n",
    "W_o = \\frac{(W_i + P_w  - K_w)}{S_w} + 1\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "* $W_i$: input image width\n",
    "* $P_w$: padding, the total number of elements to add in top and botton of the input image\n",
    "* $K_w$: kernell width\n",
    "* $S_w$: stride width. number of elements to hop \n",
    "\n",
    "\n",
    "> PS: To check just make $K_w = 1$, $P_w = 0$ and  $S_w = 1$, we expecting same image size.  \n",
    "> PS: To remember: Should be proportional to $W_i, P_w$ and $K_w, S_w$ should reduce the output size   \n",
    "> PS: You only need to check in one dimension  \n",
    "> PS: Run the formula for one small dim image like 1x3, and kernel 1x2 and 1x1 and S_w 1 and 2  \n",
    "\n",
    "The eqation is similar for the output height $H_o$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9]],\n",
       "\n",
       "       [[ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([range(10),range(10)])\n",
    "X[1,:] = X[1,:] + 1 \n",
    "X = np.reshape(X,(2,10,1)) # bs = 2\n",
    "X.shape\n",
    "X\n",
    "\n",
    "X[0,:,:].shape\n",
    "X[0,:,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 10, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected output size: (8.0, 1)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 8, 1)              4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 8, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1],\n",
       "        [ 1,  2],\n",
       "        [ 2,  3],\n",
       "        [ 3,  4],\n",
       "        [ 4,  5],\n",
       "        [ 5,  6],\n",
       "        [ 6,  7],\n",
       "        [ 7,  8],\n",
       "        [ 8,  9],\n",
       "        [ 9, 10]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Conv1d\")\n",
    "\n",
    "_kernel_init_coefs = np.array([-1, 0, 1])\n",
    "_kernel_init_coefs = tf.initializers.constant(_kernel_init_coefs)\n",
    "\n",
    "# Rememember the output of a conv layer change the output size\n",
    "# conv 1d: Output size = 1 + (in size + padding size - kernel size)/(stride size)\n",
    "\n",
    "# H W C \n",
    "X.shape\n",
    "\n",
    "in_size = X.shape[1]\n",
    "pad_size = 0\n",
    "kernel_size = 3\n",
    "stride_size = 1\n",
    "w_size = ( in_size + pad_size - kernel_size)/(stride_size) + 1  # w direction\n",
    "h_size = 1\n",
    "out_size = (w_size, h_size)\n",
    "\n",
    "print(f\"Expected output size: {out_size}\")\n",
    "model3 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(1, kernel_size=3, kernel_initializer= _kernel_init_coefs, input_shape=(10,1) )\n",
    "    ])\n",
    "\n",
    "model3.summary()\n",
    "y3 = model3.predict(X)\n",
    "y3.shape\n",
    "\n",
    "X.T\n",
    "\n",
    "# Expected all 2 values because: \n",
    "# 0*-1 + 1*0 + 2*1 = 2\n",
    "# 1*-1 + 2*0 + 3*1 = 2\n",
    "# 2*-1 + 3*0 + 4*1 = 2\n",
    "# ...\n",
    "# 7*-1 + 8*0 + 9*1 = 2\n",
    "y3.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GlobalAveragePooling1D and MaxPool1D\n",
    "\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D\n",
    "\n",
    "```python\n",
    "tf.keras.layers.MaxPool1D(\n",
    "    pool_size=2, strides=None, padding='valid', data_format='channels_last',\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\n",
    "# filters is the number of neurons\n",
    "tf.keras.layers.Conv1D(\n",
    "    filters, kernel_size, strides=1, padding='valid', data_format='channels_last',\n",
    "    dilation_rate=1, activation=None, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalAveragePooling1D\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling1d (Gl (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[4.5, 5.5]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MaxPool1D\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "max_pooling1d (MaxPooling1D) (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 5, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  2.],\n",
       "        [ 3.,  4.],\n",
       "        [ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"GlobalAveragePooling1D\")\n",
    "model1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.GlobalAveragePooling1D(input_shape=(10,1) )\n",
    "    ])\n",
    "\n",
    "model1.summary()\n",
    "y1 = model1.predict(X)\n",
    "y1.shape\n",
    "y1.T\n",
    "\n",
    "print()\n",
    "print(\"MaxPool1D\")\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.MaxPool1D(pool_size=2, input_shape=(10,1) )\n",
    "    ])\n",
    "\n",
    "model2.summary()\n",
    "y2 = model2.predict(X)\n",
    "y2.shape\n",
    "\n",
    "y2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom (lambda) Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing with Lambda Layer\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 10, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3340ff5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[4.5, 5.5]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_layer(x):\n",
    "    \n",
    "    tensor = tf.reduce_mean(x,axis=1)\n",
    "        \n",
    "    return tensor\n",
    "\n",
    "\n",
    "print(f\"Implementing with Lambda Layer\")\n",
    "model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(10,1), name=\"input_layer\"),\n",
    "        tf.keras.layers.Lambda( lambda x: custom_layer(x)  )\n",
    "    ])\n",
    "\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "X.shape\n",
    "#X\n",
    "\n",
    "y2 = model2.predict(X)\n",
    "y2.shape\n",
    "y2.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v shape: (2, 4, 3)\n",
      "Implementing with Lambda Layer\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3340ad1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.25     , 2.       , 3.25     , 0.4330127, 0.       , 1.47902  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.25     , 2.       , 3.25     , 0.4330127, 0.       , 1.47902  ],\n",
       "       [2.25     , 2.25     , 3.5      , 0.4330127, 0.4330127, 1.5      ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stat_pooling_layer(x):\n",
    "    \n",
    "    tensor_std = tf.math.reduce_std(x,axis=1)\n",
    "    tensor_mean = tf.reduce_mean(x,axis=1)\n",
    "    \n",
    "    tensor = tf.concat([tensor_mean, tensor_std], axis=1)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# batch size = 2, max seq = 4, mfcc dim = 3\n",
    "X1 = tf.constant([[[1, 2,3], [1,2, 4], [1, 2, 5], [2, 2, 1]]])\n",
    "X2 = tf.constant([[[2, 3,4], [2,2, 4], [2, 2, 5], [3, 2, 1]]])\n",
    "\n",
    "V = tf.concat([X1,X2], axis=0)\n",
    "\n",
    "print(f\"v shape: {V.shape}\")\n",
    "\n",
    "print(f\"Implementing with Lambda Layer\")\n",
    "model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(4,3), name=\"input_layer\"),\n",
    "        #tf.keras.layers.Lambda( lambda x: tf.math.reduce_std(x,axis=1) )\n",
    "        tf.keras.layers.Lambda( lambda x: stat_pooling_layer(x)  )\n",
    "    ])\n",
    "\n",
    "model2.summary()\n",
    "y2 = model2.predict(V)\n",
    "y2.shape\n",
    "\n",
    "\n",
    "# For computing the std for double checking\n",
    "# mfccs = [3,4,5,1]\n",
    "# np.mean(mfccs)\n",
    "# np.std(mfccs)\n",
    "# Expected first sequence: Mean = [ 1.25, 2.0, 3.25 ] std = [.433, 0.0, 1.4790 ]\n",
    "# y2[0,:,:] =  [ 3 means, 3 stds]\n",
    "y2[0,:] \n",
    "\n",
    "print()\n",
    "y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input are the values of x over a batch: $X = {x_1, x_2,..., x_i,..., x_m}$\n",
    "    * where $m$ is the batch size\n",
    "* Output: $y = BN_{\\gamma,\\beta}(X)$\n",
    "* Learning parameters: $\\gamma$ and $\\beta$\n",
    "* Normalization:\n",
    "\n",
    "$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i \\\\\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\\\\n",
    "z_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} \\\\\n",
    "y_i = BN_{\\gamma,\\beta}(x_i) \\equiv \\gamma z_i + \\beta\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.00372284, 4.84130788, 1.56712089, 3.46161308, 4.38194576,\n",
       "       4.47303332, 0.42522106, 0.19527392, 0.8491521 , 4.39071252])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 2.86; std: 1.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.64399874,  1.11517084, -0.72667862,  0.33904225,  0.85676291,\n",
       "        0.90800298, -1.36903888, -1.49839253, -1.13056223,  0.86169453])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.64399874,  1.11517084, -0.72667862,  0.33904225,  0.85676291,\n",
       "        0.90800298, -1.36903888, -1.49839253, -1.13056223,  0.86169453])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 0.00; std: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Batch\n",
    "X = np.random.uniform(0,5.0,size=(10))\n",
    "X\n",
    "\n",
    "print(f\"shape: {X.shape}; mean: {X.mean():.2f}; std: {X.std():.2f}\")\n",
    "\n",
    "# Coputing using tensorflow layer\n",
    "gamma = 1.0\n",
    "beta = 0.0\n",
    "epsilon = 0.0\n",
    "\n",
    "# because we did not train the layer, we are passing the mean and the variance of the batch\n",
    "Y = tf.nn.batch_normalization(X,\n",
    "                    mean = X.mean(axis=0),        # batch mean\n",
    "                    variance = X.var(axis=0),     # batch var\n",
    "                    offset = beta,scale = gamma,  # batch beta and gamma See equations  \n",
    "                    variance_epsilon = epsilon)   # batch epsilon See equations\n",
    "\n",
    "Y.numpy()\n",
    "\n",
    "# comparing with numpy\n",
    "\n",
    "Z = (X - X.mean(axis=0))/np.sqrt(X.var(axis=0) + epsilon)\n",
    "Y = gamma * Z + beta\n",
    "Y\n",
    "\n",
    "# Expectd zero mean and unit variance\n",
    "print(f\"shape: {Y.shape}; mean: {Y.mean():.2f}; std: {Y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Analyze the output of the Layers\n",
    "\n",
    "How to debbug and inspect the output of each layer of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[0],\n",
       "        [1],\n",
       "        [2]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 2, 1)              3         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1, 1)              0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_12  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "len of list of activations: 3\n",
      "Layer: 0\n",
      "output shape: (1, 2, 1)\n",
      "Activation at layer 0: [[[1.]\n",
      "  [1.]]]\n",
      "Layer: 1\n",
      "output shape: (1, 1, 1)\n",
      "Activation at layer 1: [[[1.]]]\n",
      "Layer: 2\n",
      "output shape: (1, 1)\n",
      "Activation at layer 2: [[1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "X = np.array([range(3)])\n",
    "X = np.reshape(X,(1,3,1)) # bs = 1 SGD\n",
    "X.shape\n",
    "X.T\n",
    "\n",
    "\n",
    "_kernel_init_coefs = np.array([-1, 1])\n",
    "_kernel_init_coefs = tf.initializers.constant(_kernel_init_coefs)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(1, kernel_size=2, kernel_initializer= _kernel_init_coefs, input_shape=(3,1) ),\n",
    "        tf.keras.layers.MaxPool1D(pool_size=2),\n",
    "        tf.keras.layers.GlobalAveragePooling1D()\n",
    "    ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "\n",
    "#layer_outputs\n",
    "activations = [X]\n",
    "\n",
    "A = activation_model.predict(X)\n",
    "print(f\"len of list of activations: {len(A)}\")\n",
    "for l in range(3):\n",
    "    \n",
    "    print(f\"Layer: {l}\")\n",
    "    a = A[l]\n",
    "    print(f\"output shape: {a.shape}\")\n",
    "    print(f\"Activation at layer {l}: {a.T}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Review) Optmize a function\n",
    "\n",
    "Why this example is converging? y_train does not match true model\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [2.]; b: [1.]\n",
      "true y: [3. 5. 7.]\n",
      "y_train: [1, 2, 3]\n",
      "error: [-2. -3. -4.]\n",
      "[1.73333335]\n",
      "[0.88]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    " \n",
    "x_train = [1,2,3]\n",
    "y_train = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name = 'bias')\n",
    "\n",
    "W = tf.Variable([2.0], name = 'weight')\n",
    "b = tf.Variable([1.0], name = 'bias')\n",
    "\n",
    "hypothesis = W*x_train+b\n",
    "\n",
    "\n",
    "print(f\"W: {W.numpy()}; b: {b.numpy()}\")\n",
    "print(f\"true y: {hypothesis.numpy()}\")\n",
    "print(f\"y_train: {y_train}\")\n",
    "print(f\"error: {y_train -  hypothesis.numpy()}\")\n",
    "\n",
    "@tf.function\n",
    "def cost():\n",
    "\n",
    "    y_model = W*x_train+b\n",
    "    \n",
    "    error = tf.reduce_mean(tf.square(y_train- y_model))\n",
    "    return error\n",
    "\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.02)\n",
    "\n",
    "train = optimizer.minimize(cost,var_list=[W, b])\n",
    "\n",
    "tf.print(W)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "?tf.optimizers.SGD.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install --upgrade tensorflow-probability\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized value is <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.999999> with loss 9.094947017729282e-13\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import  tensorflow_probability as tfp\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "# loss_fn = lambda: (x - 1)*(x-2)  # <== NOT working \n",
    "loss_fn = lambda: x**2 - 3*x + 2  # (x-1)(x-2)  <== return (1+2)/2 = 1.5 !?\n",
    "loss_fn = lambda: (x - 5)**2   # <== this work \n",
    "\n",
    "\n",
    "losses = tfp.math.minimize(loss_fn,\n",
    "                           num_steps=500,\n",
    "                           optimizer=tf.optimizers.SGD(learning_rate=0.1))\n",
    "\n",
    "# In TF2/eager mode, the optimization runs immediately.\n",
    "print(\"optimized value is {} with loss {}\".format(x, losses[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3015386968802827"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2.100255136478842"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06757636],\n",
       "       [ 0.13247202],\n",
       "       [-0.26493112]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: slope: 198.35337069726145; b: -8.851783526295746\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "lr: 0.10000000149011612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'SGD',\n",
       " 'learning_rate': 0.1,\n",
       " 'decay': 0.0,\n",
       " 'momentum': 0.0,\n",
       " 'nesterov': False}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Add code form intro_to_gradients.ipynb\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy import stats \n",
    "\n",
    "# Simulating data\n",
    "n_samples = 50 \n",
    "X, y, coefs = make_regression(n_samples = n_samples, \n",
    "                       n_features=1, # Regression with 1 features has 2 parameters \\theta_1,\\theta_0  \n",
    "                       n_informative=1, \n",
    "                       noise=1,\n",
    "                       coef=True, # <= mean do not return true coefs\n",
    "                       random_state=1)\n",
    "\n",
    "X.min()\n",
    "X.max()\n",
    "\n",
    "# normalize inputs: [-1.0, 1.0]\n",
    "#  np.interp: pice-wise linear interpolation\n",
    "X = np.interp(X, (X.min(), X.max()), (-1.0, 1.0))\n",
    "\n",
    "X.min()\n",
    "X.max()\n",
    "\n",
    "X.shape\n",
    "X[0:3]\n",
    "\n",
    "y.shape\n",
    "\n",
    "x = X.flatten()\n",
    "slope, intercept,_,_,_ = stats.linregress(x,y)\n",
    "print(f\"Expected: slope: {slope}; b: {intercept}\")\n",
    "\n",
    "init_guess = np.array([15, 260])\n",
    "\n",
    "W0 = tf.constant_initializer(init_guess[1])\n",
    "b0 = tf.constant_initializer(init_guess[0])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation='linear',\n",
    "                          kernel_initializer=W0,\n",
    "                          bias_initializer=b0,\n",
    "                          input_shape=(1,))\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1, \n",
    "                              momentum=0.0,\n",
    "                              nesterov=False, name='SGD'\n",
    "                             )\n",
    "\n",
    "model.compile(loss='mse', optimizer=sgd, metrics=['mse'])\n",
    "\n",
    "print('lr: {}'.format(model.optimizer.lr.numpy()))\n",
    "\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs=100, batch_size=1, verbose=0, validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: slope: 198.35337069726145; b: -8.851783526295746\n",
      "Estimated parameters: b = [[198.3256]] W = [-8.987243]\n"
     ]
    }
   ],
   "source": [
    "b, W = model.get_weights()\n",
    "\n",
    "print(f\"Expected: slope: {slope}; b: {intercept}\")\n",
    "print(f\"Estimated parameters: b = {b} W = {W}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 2, 'epochs': 100, 'steps': 50}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'mse'])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Index(['loss', 'mse'], dtype='object')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss value: loss    0.666593\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'MSE Per EPOCH')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'EPOCH')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 5.0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 30.0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model.history.params\n",
    "model.history.history.keys()\n",
    "\n",
    "metric_df = pd.DataFrame(model.history.history)\n",
    "metric_df.columns\n",
    "\n",
    "print(f\"min loss value: {metric_df[['loss']].min()}\")\n",
    "\n",
    "metric_df[['mse']].plot()\n",
    "plt.title('MSE Per EPOCH')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('MSE')\n",
    "plt.ylim(0.0,5.0)\n",
    "plt.xlim(0.0,30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "? np.interp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ref:\n",
    "* https://datascience.stackexchange.com/questions/29719/how-to-set-batch-size-steps-per-epoch-and-validation-steps\n",
    "\n",
    "\n",
    "* train_size  = batch_size x steps_per_epochs (very important to match to utilize all samples in train dataset)\n",
    "* val_size  = batch_size x validation_steps\n",
    "\n",
    "**Recipe**\n",
    "\n",
    "1. Overfit a toy train dataset: (check code)\n",
    "    * Consider overfit subset of the classes or labels for multiclass and multilabel classifiers\n",
    "    * Consider overfitt all classses or labels\n",
    "    \n",
    "1. Reduce bias\n",
    "    * Set a goal: train mse < 0.01 or train acc > 0.90\n",
    "    * Consider train longer as possible\n",
    "    * Change architecture parameters\n",
    "\n",
    "1. Tune learning rate (keep constant #epochs)\n",
    "    * Run the experiment with small number of epochs first to have an idea of the ranges\n",
    "        * Try to determine the larges lr posiible (The loss will not explode) \n",
    "    * Run with small number of lr candidates and epochs\n",
    "    * Leave running during the night (train longer)\n",
    "    \n",
    "1. Reduce variance (keep constant #epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_check_point(keras_model, chk_path, _epoch):\n",
    "    \n",
    "    checkpoint_path2 = \"checkpoints/20200514_0/keras_xv_model-0100.ckpt\"\n",
    "    checkpoint_dir2 = os.path.dirname(checkpoint_path2)\n",
    "\n",
    "    print(f\"load checkoint in folder: {checkpoint_dir2}\")\n",
    "    \n",
    "    (loss, mae) = keras_model.evaluate(x = X_train_mfcc2,y = xv_train, verbose=0)\n",
    "\n",
    "    # Check if initialize NN parameters with the parameters checkpoints\n",
    "    print (\"Before load loss = \" + str(loss))\n",
    "\n",
    "    keras_model.load_weights(checkpoint_path2.format(epoch=_epoch))\n",
    "\n",
    "    (loss, mae) = keras_model.evaluate(x = X_train_mfcc2,y = xv_train, verbose=0)\n",
    "\n",
    "    print (\"After load loss = \" + str(loss))\n",
    "        \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: Explain how to change learning rate after load parameters\n",
    "* https://www.pyimagesearch.com/2019/09/23/keras-starting-stopping-and-resuming-training/\n",
    "\n",
    "```python\n",
    "# load the checkpoint from disk\n",
    "print(\"[INFO] loading {}...\".format(args[\"model\"]))\n",
    "\n",
    "model = load_model(args[\"model\"])\n",
    "\n",
    "# update the learning rate\n",
    "print(\"[INFO] old learning rate: {}\".format(\n",
    "\t\tK.get_value(model.optimizer.lr)))\n",
    "\n",
    "K.set_value(model.optimizer.lr, 1e-2)\n",
    "print(\"[INFO] new learning rate: {}\".format(\n",
    "\t\tK.get_value(model.optimizer.lr)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number = '_1' + '/'\n",
    "\n",
    "checkpoint_dir = \"checkpoints/\" + \\\n",
    "            datetime.datetime.now().strftime(\"%Y%m%d\") + model_number\n",
    "\n",
    "checkpoint_path = checkpoint_dir + \"model-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir\n",
    "\n",
    "!mkdir -p {checkpoint_dir}\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='min',\n",
    "                                                 verbose=1, \n",
    "                                                 save_frequency = 10) # save every 5 epochs\n",
    "\n",
    "\n",
    "loss_name = 'MSE'\n",
    "metric_name = 'MAE'\n",
    "def build_model(max_seq_len, dimension):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(512, 5, activation='linear', input_shape=(max_seq_len, dimension)),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=3),\n",
    "        tf.keras.layers.Conv1D(256, 3, strides=2 ,activation='linear'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=3),\n",
    "        tf.keras.layers.Conv1D(128, 3,strides=3, activation='linear'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(512, activation='linear',name='xvectors')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 23\n",
    "\n",
    "#special_value = 99.0\n",
    "max_seq_len = X_train_mfcc2.shape[1]\n",
    "\n",
    "model = build_model(max_seq_len,dimension)\n",
    "\n",
    "print('X_train shape:{}'.format(X_train_mfcc2.shape))\n",
    "print('X_val shape:{}'.format(X_val_mfcc2.shape))\n",
    "print('y_train shape:{}'.format(xv_train.shape))\n",
    "print('y_val shape:{}'.format(xv_val.shape))\n",
    "print('#frame trimmed: {}'.format(max_seq_len))\n",
    "print('#mfcc: {}'.format(dimension))\n",
    "print('checkpoint: {}'.format(checkpoint_path))\n",
    "#print('logs: {}'.format(log_dir))\n",
    "#print('batch size: {}'.format(BATCH_SIZE))\n",
    "print('loss: {}'.format(loss_name))\n",
    "print('metric: {}'.format(metric_name))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_adam_opt = K.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss=loss_name, optimizer=_adam_opt, metrics=[metric_name])\n",
    "\n",
    "print('lr: {}'.format(model.optimizer.lr.numpy()))\n",
    "\n",
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "history = keras_model.fit(X_train_mfcc2, xv_train,\n",
    "                 epochs=EPOCHS,\n",
    "                 steps_per_epoch=30, \n",
    "                 #batch_size=BATCH_SIZE,\n",
    "                 validation_data = (X_val_mfcc2, xv_val),\n",
    "                 #validation_steps=VALIDATION_STEPS,\n",
    "                 callbacks = [cp_callback]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check learning curves\n",
    "\n",
    "metric_df = pd.DataFrame(keras_model.history.history)\n",
    "metric_df.columns\n",
    "\n",
    "print(f\"min loss value: {metric_df[['loss']].min()}\")\n",
    "\n",
    "metric_df[['loss', 'val_loss']].plot()\n",
    "plt.title('Loss & MAE Per EPOCH')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('loss or mae')\n",
    "#plt.ylim(0.0,1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation how many epochs do you need and how long is going to take\n",
    "\n",
    "def _estimate_delta_loss(loss, start_epoch, end_epoch, n_smooth):\n",
    "    \n",
    "    # loss \n",
    "    bg = start_epoch - n_smooth\n",
    "    ed = start_epoch\n",
    "    loss_start = np.mean(loss[bg:ed]) # /6.0\n",
    "    \n",
    "    bg = end_epoch - n_smooth\n",
    "    ed = end_epoch\n",
    "    loss_end = np.mean(loss[bg:ed]) # /6.0\n",
    "    \n",
    "    delta = loss_end - loss_start\n",
    "    \n",
    "    return delta\n",
    "\n",
    "start_epoch = 200\n",
    "end_epoch = 300\n",
    "n_smooth = 6\n",
    "\n",
    "delta_loss = _estimate_delta_loss(loss0, start_epoch, end_epoch, n_smooth)\n",
    "delta_val_loss = _estimate_delta_loss(val_loss0, start_epoch, end_epoch, n_smooth)\n",
    "delta_epoch = end_epoch - start_epoch\n",
    "\n",
    "print(f\"delta loss: {delta_loss:.4f} every {delta_epoch}\")\n",
    "print(f\"delta val: {delta_val_loss:.4f} every {delta_epoch}\")\n",
    "\n",
    "\n",
    "total_time_in_hour = 3*60 + 5\n",
    "\n",
    "total_epoch = 2 * 300\n",
    "time_per_100epoch = total_time_in_hour*100/total_epoch\n",
    "\n",
    "print()\n",
    "print(f\"Simple estimation\")\n",
    "for _epoch in [200, 250, 300, 600, 1000, 1500, 2000]:\n",
    "    \n",
    "    time_est = _epoch/100 * time_per_100epoch\n",
    "    loss_est = delta_loss/delta_epoch * _epoch + loss0[start_epoch]\n",
    "    val_est = delta_val_loss/delta_epoch * _epoch + val_loss0[start_epoch]\n",
    "    \n",
    "    print(f\"epoch: {_epoch}; expected loss: {loss_est:.3f}; expected val loss: {val_est:.3f} takes: {time_est:.2f} m;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune learning rate \n",
    "\n",
    "https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/\n",
    "\n",
    "* lr ranges: 1e-10 - 1e-1\n",
    "\n",
    "<img src=\"images/keras_learning_rate_finder_algorithm.png\" style=\"float:left\" width=\"300\" align=\"right\">\n",
    "\n",
    "<img src=\"images/learing_rate_finder_lr_plot.png\" style=\"float:left\" width=\"500\" align=\"right\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "start_lr = 1e-4\n",
    "number_of_lr_per_order = 15\n",
    "    \n",
    "lrs = start_lr*(10**(np.arange(EPOCHS - 1) / number_of_lr_per_order))\n",
    "\n",
    "end_lr = lrs[-1]\n",
    "\n",
    "print(f\"Learning rate search space: start: {lrs[0]}; end: {end_lr:.3f} \")\n",
    "print(f\"Searching {number_of_lr_per_order} per magnitude order\")\n",
    "\n",
    "plt.plot(lrs, 'o--')\n",
    "plt.title('Learning Rate search space');\n",
    "plt.ylabel('learning rate');\n",
    "plt.xlabel('epoch');\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogy(lrs, 'o--')\n",
    "plt.title('Learning Rate search space');\n",
    "plt.ylabel('learning rate');\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "_loss = history.history[\"loss\"]\n",
    "\n",
    "print(f\"Checking dimensions matches: lrs: {len(lrs)}; len history: {_loss}\")\n",
    "\n",
    "# x =  lr and y = loss\n",
    "plt.semilogx(lrs,_loss ,'o--' )\n",
    "#plt.axis([start_lr, end_lr, 0, 5])\n",
    "\n",
    "lrs[5:10]\n",
    "_loss[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Checking lr candidates\n",
    "\n",
    "EPOCHS = 55\n",
    "\n",
    "lr_candidates = [ 0.01, 0.006,0.001, 0.00045 ]\n",
    "\n",
    "lr_curves = list()\n",
    "for lr in lr_candidates:\n",
    "    \n",
    "    print(f\"Training with lr: {lr}\")\n",
    "    model = xv_model(max_seq_len,dimension)\n",
    "    \n",
    "    _adam_opt = K.optimizers.Adam(lr=lr)\n",
    "    model.compile(loss=loss_name, optimizer=_adam_opt, metrics=[metric_name])\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_mfcc2, xv_train,\n",
    "                 epochs=EPOCHS,\n",
    "                 steps_per_epoch=100, #EVALUATION_INTERVAL,\n",
    "                 #batch_size=BATCH_SIZE,\n",
    "                 validation_data = (X_val_mfcc2, xv_val),\n",
    "                 #validation_steps=VALIDATION_STEPS\n",
    "                )\n",
    "    \n",
    "    lr_curves.append((lr, history.history[\"loss\"], history.history[\"val_loss\"]  ))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the results\n",
    "epochs = range(EPOCHS)\n",
    "\n",
    "for lr, loss, val_loss in lr_curves:\n",
    "    \n",
    "    print(f\"lr: {lr}; loss size: {len(loss)}; start: {loss[0]:.3f}; end: {loss[-1]:.3f}; epoch size: {len(epochs)}\")\n",
    "    \n",
    "    _ = plt.plot(epochs,loss,  label='lr: {}'.format(lr));\n",
    "\n",
    "\n",
    "_ = plt.title('Learning curves');\n",
    "_ = plt.ylabel('loss');\n",
    "_ = plt.xlabel('epoch');\n",
    "#_ = plt.ylim(0.0,1.5)\n",
    "_ = plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue train from checkpoint (transfer learning)\n",
    "\n",
    "* If you load an external model can be transfer learning tehcnique\n",
    "* Ability to change the learning rate or optmizer and start train from the last checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **Training set**: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).\n",
    "- **Test set**: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:20:05.450391Z",
     "start_time": "2019-05-15T22:20:05.405403Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    train_dataset = h5py.File('data/train_signs.h5', \"r\")\n",
    "    \n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('data/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_orig.shape\n",
    "\n",
    "type(Y_train_orig)\n",
    "\n",
    "Y_train_orig[0,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:20:06.228960Z",
     "start_time": "2019-05-15T22:20:06.076697Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.shape(classes))\n",
    "print(classes)\n",
    "\n",
    "# See example of image\n",
    "index = 0\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* preprocessing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:48:24.233605Z",
     "start_time": "2019-05-14T23:48:24.164274Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "\n",
    "# Flatten the training and test images\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_flatten / 255.\n",
    "X_test = X_test_flatten / 255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6)\n",
    "\n",
    "print(\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[1]))\n",
    "\n",
    "print()\n",
    "print('64*64*3 = 12288')\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))\n",
    "\n",
    "print()\n",
    "print (\"Y_test (1st 5) = \\n\" + str(np.squeeze(Y_test[:,0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convolutional (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:22:57.512166Z",
     "start_time": "2019-05-15T03:22:57.488726Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datasets import flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.351px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

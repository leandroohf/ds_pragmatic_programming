{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-11T18:09:18.303884Z",
     "start_time": "2018-09-11T18:09:18.300979Z"
    }
   },
   "source": [
    "# Data Science Pragmatic programming in NLP\n",
    "\n",
    "\n",
    "**TODO** REVIEW\n",
    "\n",
    "Add reference and code to spacy\n",
    "\n",
    "Split in sections\n",
    "\n",
    "1. Notes and terms\n",
    "\n",
    "1. Text processing\n",
    "    * regularexpression\n",
    "    * string indexing schema\n",
    "    \n",
    "1. Text representations\n",
    "    1. bag of words\n",
    "    1. Embeedings\n",
    "        * word2vec\n",
    "        * BERT\n",
    "        * GPT-2 and GPT-3\n",
    "1. NLP tasks\n",
    "    * NER\n",
    "    * Part of Speech\n",
    "    * Text summarization (The main idea of text)\n",
    "    * chat bot\n",
    "1. Languages\n",
    "    * Find code related to other languages not English like stop word or Lemmatization for Portuguese   \n",
    "\n",
    "1. speech to text (Maybe)\n",
    "\n",
    "refs:\n",
    "* https://medium.com/towards-artificial-intelligence/nlp-zero-to-hero-with-python-2df6fcebff6e\n",
    "\n",
    "Code snippets for quick consult (copy n paste)\n",
    "\n",
    "* NLP is really common because the majority of data on internet is text. Text is found everywhere indeed.\n",
    "* It is pretty common to have un labeled data\n",
    "* It is pretty common to have memory issue \n",
    "* Normally the histograms of word count is long tails distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T22:25:27.597585Z",
     "start_time": "2019-05-08T22:25:27.185743Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from nltk.corpus import movie_reviews as reviews\n",
    "import utils as ut\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T21:14:19.686361Z",
     "start_time": "2019-05-07T21:14:19.568970Z"
    }
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP notes and terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short notes about string encode and string literals in python\n",
    "\n",
    "ref:https://stackoverflow.com/questions/2081640/what-exactly-do-u-and-r-string-flags-do-and-what-are-raw-string-literals\n",
    "Python 2 behavior is different form Python 3 that is more simple\n",
    "\n",
    "\n",
    "* Python string literals\n",
    "\n",
    "    * r'This is a string with / backslash.' (There is no escape character) \n",
    "        * The syntax \"r\" exist to make easy work with regex that is heavy in backslash \n",
    "    * u'This is a string with // backslash.' \"/\" is escape character \n",
    "    * b'This is bytecode string'. sequence of byte , similar to unicode in python 2.*. Bytecode are **machine readable**\n",
    "         * you can encode and decode bytecodes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing size to show there is difference\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('comparing size to show there is difference')\n",
    "sys.getsizeof('ciao')\n",
    "sys.getsizeof(u'ciao')\n",
    "sys.getsizeof(r'ciao')\n",
    "\n",
    "sys.getsizeof(b'ciao')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unicode**\n",
    "\n",
    "refs:\n",
    "*  https://www.geeksforgeeks.org/byte-objects-vs-string-python/\n",
    "* https://stackoverflow.com/questions/18034272/python-str-vs-unicode-types/18034409\n",
    "\n",
    "The purpose of all this is to provide a means to unambiguously refer to a each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding successful: b'GeeksforGeeks'\n",
      "Decoding successful\n",
      "\n",
      "b'GeeksforGeeks'\n",
      "GeeksforGeeks\n"
     ]
    }
   ],
   "source": [
    "# initialising a String  \n",
    "a = 'GeeksforGeeks'\n",
    "sys.getsizeof(a)  \n",
    "\n",
    "# initialising a byte object \n",
    "c = b'GeeksforGeeks'\n",
    "sys.getsizeof(c)\n",
    "\n",
    "# using encode() to encode the String \n",
    "# encoded version of a is stored in d \n",
    "# using ASCII mapping \n",
    "d = a.encode('ASCII') \n",
    "  \n",
    "# checking if a is converted to bytes or not \n",
    "print(f\"Encoding successful: {d}\" if d ==c else \"Encoding Unsuccessful\" )\n",
    "    \n",
    "# using decode() to decode the Byte object \n",
    "# decoded version of c is stored in d \n",
    "# using ASCII mapping \n",
    "d = c.decode('ASCII') \n",
    "print(\"Decoding successful\" if d ==a else \"Decoding Unsuccessful\" )\n",
    "\n",
    "print()\n",
    "print(c)\n",
    "print(c.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T21:07:04.863540Z",
     "start_time": "2019-03-07T21:07:04.855274Z"
    }
   },
   "source": [
    "### NLP terms \n",
    "\n",
    "refs: https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html\n",
    "\n",
    "1. corpora or corpus \n",
    "    * collection of text\n",
    "1. semantics\n",
    "    * semantcs = meaning of the text\n",
    "1. stop words\n",
    "\n",
    "    * They are words to be removed because they do not add information or contents to a sentence: Ex: \"the\", \"and\" and \"a\"\n",
    "    \n",
    "    * It is good to remove stop words:\n",
    "        * Text Classification\n",
    "            * Spam Filtering\n",
    "            * Language Classification\n",
    "            * Genre Classification\n",
    "        * Caption Generation\n",
    "        * Auto-Tag Generation\n",
    "\n",
    "    * AVOID remove stop words:\n",
    "        * **Small datastes** stop words can play a key role in connecting semantic meaning\n",
    "        * Machine Translation\n",
    "        * Language modelling \n",
    "        * Text Summarization\n",
    "\n",
    "1. Tokenization\n",
    "\n",
    "    * The process to convert long sentences in small pices called tokens (a word vsn be a token)\n",
    "\n",
    "1. n-gram: bi-grams and tri-grams\n",
    "1. skip_gram\n",
    "1. Stemming\n",
    "\n",
    "    * process to reduce worfds variations: run, runs, runnig and ran is map to run. \"tradition\" and \"traditional\" has the sma stem \"tradit\"\n",
    "    \n",
    "1. Lemmatization\n",
    "\n",
    "    * same idea as Stemming but capture a little bit of meaning. Ex: better, good, awsome can be map to good\n",
    "    \n",
    "1. simillarity measures\n",
    "\n",
    "    * jaccard\n",
    "    * leveintein or edit distance\n",
    "    \n",
    "1. tfidf = tf * idf\n",
    "\n",
    "    * tf: term frequency, which measures how frequently a term occurs in a document. Is a local metric and measure how important is the term for that sentence. it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length.\n",
    "    \n",
    "    * idf: inverse of docment (document contains the term) frequency. Is a global metric (relative to the corpus). Gives how unique is an specific term\n",
    "\n",
    "$\n",
    "idf(t) = log \\left ( \\frac{\\text{Number of docs}}{\\text{Number of documents with term t in it}} \\right) \n",
    "$\n",
    "\n",
    "\n",
    "sciki-learn definition of idf differs form the majority of text books.\n",
    "See http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "$\n",
    "idf(t) = log ( \\frac{1 + n}{ 1+ df(t)} ) + 1\n",
    "$\n",
    "\n",
    "And then idf is nomralized using eucledian norm\n",
    "\n",
    "$\n",
    "v_{norm} = \\frac{v_k}{\\sqrt{ \\sum_i v_i^2 }}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note about large vocabulary and memory issue \n",
    "\n",
    "Because is so common to have a large vocabulary it is normal to have memmory issue. \n",
    "\n",
    "\n",
    "You can minimize the memory problem and also is good for modelling by minimizing the corpus vocabulary. Things you can do:\n",
    "\n",
    "1. Clean and remove stop words\n",
    "\n",
    "1. Stemming and Lemmatization \n",
    "    \n",
    "    Reduce the total number of unique words in the dictionary. As a result, the number of columns in the document-word matrix (created by CountVectorizer in the next step)\n",
    "\n",
    "\n",
    "Another way is working with Sparse matrix and machine learning implementation that support Sparse matrix like the sparse matrix implemented in **scipy.sparse** and the machine learnig in scikit-learn.\n",
    "\n",
    "\n",
    "refs: https://docs.scipy.org/doc/scipy/reference/sparse.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.1\n",
      "  (0, 1)\t2.0\n",
      "  (1, 2)\t3.0\n",
      "  (2, 0)\t4.0\n",
      "  (2, 2)\t5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.1, 2. , 0. ],\n",
       "       [0. , 0. , 3. ],\n",
       "       [4. , 0. , 5. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 64 bytes\n",
      "size: 184 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0],\n",
       "        [0, 1],\n",
       "        [1, 2],\n",
       "        [2, 0],\n",
       "        [2, 2]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.1, 2. , 3. , 4. , 5. ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0. , 0. , 1.1],\n",
       "        [0. , 1. , 2. ],\n",
       "        [1. , 2. , 3. ],\n",
       "        [2. , 0. , 4. ],\n",
       "        [2. , 2. , 5. ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "sA = csr_matrix([[1.1, 2., 0.0], [0, 0, 3.0], [4, 0.0, 5]])\n",
    "sA.count_nonzero()\n",
    "print(sA)\n",
    "\n",
    "# convert to dense numpy array\n",
    "A = sA.toarray()\n",
    "A\n",
    "\n",
    "# Return the size of an object in bytes\n",
    "print('size: {} bytes'.format(sys.getsizeof(sA)))\n",
    "print('size: {} bytes'.format(sys.getsizeof(A)))\n",
    "\n",
    "## All matrix manipulation is still avaliable\n",
    "B = np.array([[1,0,0],[0,1,0]])\n",
    "B\n",
    "\n",
    "# convert numpy arary to sparse\n",
    "sB = csr_matrix(B) \n",
    "sB\n",
    "print(type(sB))\n",
    "print(sB)\n",
    "\n",
    "## TODO How to get list of coordinates and values like in print\n",
    "coo = sA.tocoo()\n",
    "\n",
    "\n",
    "## index\n",
    "indices = np.mat([coo.row, coo.col]).transpose()\n",
    "\n",
    "indices\n",
    "\n",
    "## values\n",
    "coo.data\n",
    "\n",
    "## all together\n",
    "np.mat([coo.row, coo.col, coo.data]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### String manipuations\n",
    "\n",
    "* indexing schema\n",
    "* lower\n",
    "* contains \n",
    "* iterate over sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pandas string column manipulations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "\n",
    "* word2vec\n",
    "* BERT\n",
    "*GPT-2 and GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get some data\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = [reviews.raw(fileid) for fileid in reviews.fileids()]\n",
    "y = [reviews.categories(fileid)[0] for fileid in reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'the happy bastard\\'s quick movie review \\ndamn that y2k bug . \\nit\\'s got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . \\nlittle do they know the power within . . . \\ngoing for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . \\nwe don\\'t know why the crew was really out in the middle of nowhere , we don\\'t know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don\\'t know why donald sutherland is stumbling around drunkenly throughout . \\nhere , it\\'s just \" hey , let\\'s chase these people around with some robots \" . \\nthe acting is below average , even from the likes of curtis . \\nyou\\'re more likely to get a kick out of her work in halloween h20 . \\nsutherland is wasted and baldwin , well , he\\'s acting like a baldwin , of course . \\nthe real star here are stan winston\\'s robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone\\'s brain . \\nso , if robots and body parts really turn you on , here\\'s your movie . \\notherwise , it\\'s pretty much a sunken ship of a movie . \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]\n",
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize data\n",
    "\n",
    "\n",
    "* bag of words\n",
    "* tfidf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_utils as tut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# * *************************************************************************\u001b[39;49;00m\n",
      "\u001b[37m#   Programmer[s]: Leandro Fernandes\u001b[39;49;00m\n",
      "\u001b[37m#   email: leandroohf@gmail.com\u001b[39;49;00m\n",
      "\u001b[37m#   Program: text_utils\u001b[39;49;00m\n",
      "\u001b[37m#   Commentary: My utils for helping with text processing\u001b[39;49;00m\n",
      "\u001b[37m#   Date: February 6, 2020\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m#   The author believes that share code and knowledge is awesome.\u001b[39;49;00m\n",
      "\u001b[37m#   Feel free to share and modify this piece of code. But don't be\u001b[39;49;00m\n",
      "\u001b[37m#   impolite and remember to cite the author and give him his credits.\u001b[39;49;00m\n",
      "\u001b[37m# * *************************************************************************\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mstring\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mnltk\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m word_tokenize\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mnltk.tokenize\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m RegexpTokenizer\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.feature_extraction.text\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m CountVectorizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.feature_extraction.text\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TfidfVectorizer\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m List, Optional\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_max_word_len\u001b[39;49;00m(sentence: \u001b[36mstr\u001b[39;49;00m):\n",
      "\n",
      "    s1 = word_tokenize(sentence)\n",
      "    sentence_len = \u001b[36mlen\u001b[39;49;00m(s1)\n",
      "\n",
      "    len_of_largest_word = \u001b[34m0\u001b[39;49;00m\n",
      "    largest_word = \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    \u001b[37m# O(n)\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m word \u001b[35min\u001b[39;49;00m s1:\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(word) > len_of_largest_word):\n",
      "\n",
      "            len_of_largest_word = \u001b[36mlen\u001b[39;49;00m(word)\n",
      "            largest_word = word\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m largest_word, len_of_largest_word, sentence_len\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_corpus_table\u001b[39;49;00m(corpus: \u001b[36mlist\u001b[39;49;00m) -> pd.DataFrame:\n",
      "\n",
      "    corpus_df = pd.DataFrame({\u001b[33m\"\u001b[39;49;00m\u001b[33mdoc\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: corpus})\n",
      "\n",
      "    corpus_df[\u001b[33m'\u001b[39;49;00m\u001b[33mlargest_word\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = corpus_df.doc.map(\u001b[34mlambda\u001b[39;49;00m s: get_max_word_len(s)[\u001b[34m0\u001b[39;49;00m])\n",
      "    corpus_df[\u001b[33m'\u001b[39;49;00m\u001b[33mlen_of_largest_word\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = corpus_df.doc.map(\u001b[34mlambda\u001b[39;49;00m s: get_max_word_len(s)[\u001b[34m1\u001b[39;49;00m])\n",
      "    corpus_df[\u001b[33m'\u001b[39;49;00m\u001b[33mdoc_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = corpus_df.doc.map(\u001b[34mlambda\u001b[39;49;00m s: get_max_word_len(s)[\u001b[34m2\u001b[39;49;00m])\n",
      "\n",
      "    tokenizer = RegexpTokenizer(\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\u001b[33mw+\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    corpus_df[\u001b[33m'\u001b[39;49;00m\u001b[33mwords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = corpus_df.doc.map(tokenizer.tokenize)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m corpus_df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msummarize_corpus\u001b[39;49;00m(corpus_df: pd.DataFrame ):\n",
      "\n",
      "    all_words = [word \u001b[34mfor\u001b[39;49;00m tokens \u001b[35min\u001b[39;49;00m corpus_df[\u001b[33m\"\u001b[39;49;00m\u001b[33mwords\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m word \u001b[35min\u001b[39;49;00m tokens]\n",
      "    n_words = \u001b[36mlen\u001b[39;49;00m(all_words)\n",
      "    vocab_size = \u001b[36mlen\u001b[39;49;00m(\u001b[36mset\u001b[39;49;00m(all_words))\n",
      "\n",
      "    n_docs = corpus_df.shape[\u001b[34m0\u001b[39;49;00m]\n",
      "\n",
      "    idx = corpus_df[\u001b[33m\"\u001b[39;49;00m\u001b[33mlen_of_largest_word\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].idxmax()\n",
      "    largest_word, len_of_largest_word = corpus_df[[\u001b[33m'\u001b[39;49;00m\u001b[33mlargest_word\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[33m'\u001b[39;49;00m\u001b[33mlen_of_largest_word\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]].iloc[idx]\n",
      "\n",
      "    min_doc_len = corpus_df[\u001b[33m\"\u001b[39;49;00m\u001b[33mdoc_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].min()\n",
      "    median_doc_len = corpus_df[\u001b[33m\"\u001b[39;49;00m\u001b[33mdoc_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].median()\n",
      "    mean_doc_len = corpus_df[\u001b[33m\"\u001b[39;49;00m\u001b[33mdoc_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].mean()\n",
      "    max_doc_len = corpus_df[\u001b[33m\"\u001b[39;49;00m\u001b[33mdoc_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].max()\n",
      "\n",
      "    word_summary = pd.DataFrame({\u001b[33m'\u001b[39;49;00m\u001b[33mn_words\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: n_words, \u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: vocab_size,\n",
      "                                 \u001b[33m'\u001b[39;49;00m\u001b[33mlargest_word\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: largest_word, \u001b[33m'\u001b[39;49;00m\u001b[33mlen_of_largest_word\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: len_of_largest_word }, index=[\u001b[34m0\u001b[39;49;00m])\n",
      "\n",
      "    doc_summary = pd.DataFrame({\u001b[33m'\u001b[39;49;00m\u001b[33mn_docs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: n_docs, \u001b[33m'\u001b[39;49;00m\u001b[33mmin_doc_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: min_doc_len,\u001b[33m'\u001b[39;49;00m\u001b[33mmean_doc_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: mean_doc_len,\n",
      "                                \u001b[33m'\u001b[39;49;00m\u001b[33mmedian_doc_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: median_doc_len, \u001b[33m'\u001b[39;49;00m\u001b[33mmax_doc_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: max_doc_len}, index=[\u001b[34m0\u001b[39;49;00m])\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m word_summary, doc_summary\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_word_frequency_table\u001b[39;49;00m(corpus: \u001b[36mlist\u001b[39;49;00m, *args, **kargs) -> pd.DataFrame:\n",
      "    \u001b[33m\"\"\" Returns DataFrame with the number of times a word is mentioned in all documents.\u001b[39;49;00m\n",
      "\u001b[33m    Ex: word_freq = 5, word = 'film' means that the word 'film' was mentioned 5 times in all documents\u001b[39;49;00m\n",
      "\u001b[33m    word_freq is not tf because tf is relative to the document or sentence. This is global  word frequency \u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "\n",
      "    vectorizer = CountVectorizer(*args, binary=\u001b[36mFalse\u001b[39;49;00m, **kargs )\n",
      "\n",
      "    \u001b[37m# sparse matrix\u001b[39;49;00m\n",
      "    \u001b[37m# doc x word count matrix\u001b[39;49;00m\n",
      "    \u001b[37m# #docs, #words = bag_of_words.shape\u001b[39;49;00m\n",
      "    bag_of_words = vectorizer.fit_transform(corpus)\n",
      "\n",
      "    word_freq = bag_of_words.sum(axis=\u001b[34m0\u001b[39;49;00m)\n",
      "\n",
      "    word_freq_df = pd.DataFrame({\u001b[33m'\u001b[39;49;00m\u001b[33mword_freq\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: word_freq.tolist()[\u001b[34m0\u001b[39;49;00m]})\n",
      "    words = vectorizer.get_feature_names()\n",
      "\n",
      "    word_freq_df[\u001b[33m'\u001b[39;49;00m\u001b[33mword\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = word_freq_df.apply(\u001b[34mlambda\u001b[39;49;00m r: words[r.name],axis=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# sort by frequency of word\u001b[39;49;00m\n",
      "    word_freq_df.sort_values(by=\u001b[33m'\u001b[39;49;00m\u001b[33mword_freq\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,ascending=\u001b[36mFalse\u001b[39;49;00m, inplace=\u001b[36mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m word_freq_df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_doc_frequency_by_word_table\u001b[39;49;00m(corpus: \u001b[36mlist\u001b[39;49;00m, *args, **kargs) -> pd.DataFrame:\n",
      "    \u001b[33m\"\"\" Returns DataFrame with the number of documents that contains the words (df) and the idf  per word\u001b[39;49;00m\n",
      "\u001b[33m    Ex: doc_freq = 5 (df), word = 'film' means that there are 5 documents that contains the word film in the corpus\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    vectorizer = CountVectorizer(*args, binary=\u001b[36mTrue\u001b[39;49;00m, **kargs )\n",
      "\n",
      "    n_docs = \u001b[36mlen\u001b[39;49;00m(corpus)\n",
      "\n",
      "    \u001b[37m# sparse matrix\u001b[39;49;00m\n",
      "    \u001b[37m# doc x word matrix\u001b[39;49;00m\n",
      "    \u001b[37m# #docs, #words = X.shape\u001b[39;49;00m\n",
      "    doc_word_matrix = vectorizer.fit_transform(corpus)\n",
      "\n",
      "    doc_freq = doc_word_matrix.sum(axis=\u001b[34m0\u001b[39;49;00m)\n",
      "\n",
      "    doc_freq_df = pd.DataFrame({\u001b[33m'\u001b[39;49;00m\u001b[33mdoc_freq\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: doc_freq.tolist()[\u001b[34m0\u001b[39;49;00m]})\n",
      "\n",
      "    words = vectorizer.get_feature_names()\n",
      "\n",
      "    doc_freq_df[\u001b[33m'\u001b[39;49;00m\u001b[33mdf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = doc_freq_df.doc_freq.map(\u001b[34mlambda\u001b[39;49;00m x: x/n_docs)\n",
      "    doc_freq_df[\u001b[33m'\u001b[39;49;00m\u001b[33mword\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = doc_freq_df.apply(\u001b[34mlambda\u001b[39;49;00m r: words[r.name],axis=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# sort by frequency of word\u001b[39;49;00m\n",
      "    doc_freq_df.sort_values(by=\u001b[33m'\u001b[39;49;00m\u001b[33mdoc_freq\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,ascending=\u001b[36mFalse\u001b[39;49;00m, inplace=\u001b[36mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m doc_freq_df\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_tfidf_table\u001b[39;49;00m(corpus: \u001b[36mlist\u001b[39;49;00m, *args, **kargs) -> pd.DataFrame:\n",
      "    \u001b[33m\"\"\"Returns statistics for tfidf in a corpus per word\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    vectorizer = TfidfVectorizer(*args, **kargs )\n",
      "\n",
      "    \u001b[37m# sparse matrix\u001b[39;49;00m\n",
      "    \u001b[37m# doc x word matrix\u001b[39;49;00m\n",
      "    \u001b[37m# #docs, #words = X.shape\u001b[39;49;00m\n",
      "    bag_of_tfidf = vectorizer.fit_transform(corpus)\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(bag_of_tfidf.shape)\n",
      "\n",
      "    words = vectorizer.get_feature_names()\n",
      "\n",
      "    tfidf_df = pd.DataFrame({\u001b[33m'\u001b[39;49;00m\u001b[33mwords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: words,\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mmin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: bag_of_tfidf.min(axis=\u001b[34m0\u001b[39;49;00m),\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mmean\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: bag_of_tfidf.mean(axis=\u001b[34m0\u001b[39;49;00m),\n",
      "                             \u001b[37m#'median': bag_of_tfidf.median(axis=0), # TODO; Check this later\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: bag_of_tfidf.max(axis=\u001b[34m0\u001b[39;49;00m)})\n",
      "\n",
      "\n",
      "    \u001b[37m# sort by frequency of word\u001b[39;49;00m\n",
      "    tfidf_df.sort_values(by=\u001b[33m'\u001b[39;49;00m\u001b[33mmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,ascending=\u001b[36mFalse\u001b[39;49;00m, inplace=\u001b[36mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m tfidf_df\n",
      "\n",
      "\n",
      "\u001b[37m# Create function using string.punctuation to remove all punctuation\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mremove_punctuation\u001b[39;49;00m(sentence: \u001b[36mstr\u001b[39;49;00m) -> \u001b[36mstr\u001b[39;49;00m:\n",
      "    \u001b[34mreturn\u001b[39;49;00m sentence.translate(\u001b[36mstr\u001b[39;49;00m.maketrans(\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, string.punctuation))\n",
      "\n",
      "\u001b[37m# TODO\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mclean_text_data\u001b[39;49;00m(sentences: \u001b[36mstr\u001b[39;49;00m, large_words: \u001b[36mint\u001b[39;49;00m , my_stopped_words = Optional[List[\u001b[36mstr\u001b[39;49;00m]]) -> \u001b[36mstr\u001b[39;49;00m:\n",
      "\n",
      "    \u001b[34mpass\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize text_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Get the big picture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 54.3 ms, total: 21.7 s\n",
      "Wall time: 21.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>largest_word</th>\n",
       "      <th>len_of_largest_word</th>\n",
       "      <th>doc_len</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>disappearances</td>\n",
       "      <td>14</td>\n",
       "      <td>841</td>\n",
       "      <td>[plot, two, teen, couples, go, to, a, church, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
       "      <td>strangeness</td>\n",
       "      <td>11</td>\n",
       "      <td>291</td>\n",
       "      <td>[the, happy, bastard, s, quick, movie, review,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>unfortunately</td>\n",
       "      <td>13</td>\n",
       "      <td>562</td>\n",
       "      <td>[it, is, movies, like, these, that, make, a, j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc    largest_word  \\\n",
       "0  plot : two teen couples go to a church party ,...  disappearances   \n",
       "1  the happy bastard's quick movie review \\ndamn ...     strangeness   \n",
       "2  it is movies like these that make a jaded movi...   unfortunately   \n",
       "\n",
       "   len_of_largest_word  doc_len  \\\n",
       "0                   14      841   \n",
       "1                   11      291   \n",
       "2                   13      562   \n",
       "\n",
       "                                               words  \n",
       "0  [plot, two, teen, couples, go, to, a, church, ...  \n",
       "1  [the, happy, bastard, s, quick, movie, review,...  \n",
       "2  [it, is, movies, like, these, that, make, a, j...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "corpus_df = tut.create_corpus_table(X)\n",
    "corpus_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 136 ms, sys: 4 ms, total: 140 ms\n",
      "Wall time: 139 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_docs</th>\n",
       "      <th>min_doc_len</th>\n",
       "      <th>mean_doc_len</th>\n",
       "      <th>median_doc_len</th>\n",
       "      <th>max_doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>18</td>\n",
       "      <td>762.5195</td>\n",
       "      <td>712.5</td>\n",
       "      <td>2753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_docs  min_doc_len  mean_doc_len  median_doc_len  max_doc_len\n",
       "0    2000           18      762.5195           712.5         2753"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_summary, doc_summary = tut.summarize_corpus(corpus_df)\n",
    "\n",
    "doc_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>largest_word</th>\n",
       "      <th>len_of_largest_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1336782</td>\n",
       "      <td>39696</td>\n",
       "      <td>dead-guy-seems-to-have-come-back-to-life-but-t...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_words  vocab_size                                       largest_word  \\\n",
       "0  1336782       39696  dead-guy-seems-to-have-come-back-to-life-but-t...   \n",
       "\n",
       "   len_of_largest_word  \n",
       "0                   79  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Work with toy example** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot: attempt movie, movie',\n",
       " 'plot: it is movie like these that make a jaded movie viewer thankful for the invention of the timex indiglo watch',\n",
       " 'plot: it is clear that the film']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy = [\n",
    "        'plot: attempt movie, movie',\n",
    "        'plot: it is movie like these that make a jaded movie viewer thankful for the invention of the timex indiglo watch',\n",
    "        'plot: it is clear that the film'\n",
    "         ]\n",
    "X_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>attempt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>indiglo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>invention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>jaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>thankful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word_freq       word\n",
       "8           4      movie\n",
       "9           3       plot\n",
       "0           1    attempt\n",
       "1           1      clear\n",
       "2           1       film\n",
       "3           1    indiglo\n",
       "4           1  invention\n",
       "5           1      jaded\n",
       "6           1       like\n",
       "7           1       make\n",
       "10          1   thankful"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_df = tut.get_word_frequency_table(X_toy, stop_words='english')\n",
    "word_freq_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>13085</td>\n",
       "      <td>9517</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22959</td>\n",
       "      <td>5771</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20351</td>\n",
       "      <td>3690</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19034</td>\n",
       "      <td>2905</td>\n",
       "      <td>just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14895</td>\n",
       "      <td>2411</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35358</td>\n",
       "      <td>2411</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33490</td>\n",
       "      <td>2169</td>\n",
       "      <td>story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5961</td>\n",
       "      <td>2020</td>\n",
       "      <td>character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5973</td>\n",
       "      <td>1859</td>\n",
       "      <td>characters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38204</td>\n",
       "      <td>1693</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21167</td>\n",
       "      <td>1642</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq        word\n",
       "13085       9517        film\n",
       "22959       5771       movie\n",
       "20351       3690        like\n",
       "19034       2905        just\n",
       "14895       2411        good\n",
       "35358       2411        time\n",
       "33490       2169       story\n",
       "5961        2020   character\n",
       "5973        1859  characters\n",
       "38204       1693         way\n",
       "21167       1642        make"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_df = tut.get_word_frequency_table(X, stop_words='english')\n",
    "word_freq_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f029ca18790>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f029c6cf790>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# It is long tail distribution\n",
    "word_freq_df.hist(column=\"word_freq\",bins=30)\n",
    "\n",
    "# Just see ROI\n",
    "word_freq_df[(word_freq_df.word_freq < 500) & (word_freq_df.word_freq > 50)].hist(column=\"word_freq\",bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>df</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>attempt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_freq        df     word\n",
       "9         3  1.000000     plot\n",
       "8         2  0.666667    movie\n",
       "0         1  0.333333  attempt\n",
       "1         1  0.333333    clear\n",
       "2         1  0.333333     film"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_freq_df = tut.get_doc_frequency_by_word_table(X_toy, stop_words='english')\n",
    "\n",
    "doc_freq_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fixme: Code to summarize sparse matrix is broken after change form numpy to sparse matrix representation \n",
    "tfidf_df = tut.get_tfidf_table(X_toy, stop_words='english')\n",
    "\n",
    "tfidf_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Help to debug the function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# sparse matrix\n",
    "# doc x word matrix\n",
    "# #docs, #words = X.shape\n",
    "bag_of_tfidf = vectorizer.fit_transform(X_toy)\n",
    "\n",
    "print(bag_of_tfidf.shape)\n",
    "\n",
    "bag_of_tfidf.toarray()\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_df = pd.DataFrame({'words': words,\n",
    "                         'min': bag_of_tfidf.min(axis=0),\n",
    "                         'mean': bag_of_tfidf.mean(axis=0),\n",
    "                         #'median': bag_of_tfidf.median(axis=0),\n",
    "                         'max': bag_of_tfidf.max(axis=0)})\n",
    "\n",
    "\n",
    "\n",
    "# sort by frequency of word\n",
    "tfidf_df.sort_values(by='max',ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'happy',\n",
       " 'bastard',\n",
       " 's',\n",
       " 'quick',\n",
       " 'movie',\n",
       " 'review',\n",
       " 'damn',\n",
       " 'that',\n",
       " 'y2k',\n",
       " 'bug']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[['plot',\n",
       "  'two',\n",
       "  'teen',\n",
       "  'couples',\n",
       "  'go',\n",
       "  'to',\n",
       "  'a',\n",
       "  'church',\n",
       "  'party',\n",
       "  'drink',\n",
       "  'and',\n",
       "  'then',\n",
       "  'drive',\n",
       "  'they',\n",
       "  'get',\n",
       "  'into',\n",
       "  'an',\n",
       "  'accident',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'guys',\n",
       "  'dies',\n",
       "  'but',\n",
       "  'his',\n",
       "  'girlfriend',\n",
       "  'continues',\n",
       "  'to',\n",
       "  'see',\n",
       "  'him',\n",
       "  'in',\n",
       "  'her',\n",
       "  'life',\n",
       "  'and',\n",
       "  'has',\n",
       "  'nightmares',\n",
       "  'what',\n",
       "  's',\n",
       "  'the',\n",
       "  'deal',\n",
       "  'watch',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'and',\n",
       "  'sorta',\n",
       "  'find',\n",
       "  'out',\n",
       "  'critique',\n",
       "  'a',\n",
       "  'mind',\n",
       "  'fuck',\n",
       "  'movie',\n",
       "  'for',\n",
       "  'the',\n",
       "  'teen',\n",
       "  'generation',\n",
       "  'that',\n",
       "  'touches',\n",
       "  'on',\n",
       "  'a',\n",
       "  'very',\n",
       "  'cool',\n",
       "  'idea',\n",
       "  'but',\n",
       "  'presents',\n",
       "  'it',\n",
       "  'in',\n",
       "  'a',\n",
       "  'very',\n",
       "  'bad',\n",
       "  'package',\n",
       "  'which',\n",
       "  'is',\n",
       "  'what',\n",
       "  'makes',\n",
       "  'this',\n",
       "  'review',\n",
       "  'an',\n",
       "  'even',\n",
       "  'harder',\n",
       "  'one',\n",
       "  'to',\n",
       "  'write',\n",
       "  'since',\n",
       "  'i',\n",
       "  'generally',\n",
       "  'applaud',\n",
       "  'films',\n",
       "  'which',\n",
       "  'attempt',\n",
       "  'to',\n",
       "  'break',\n",
       "  'the',\n",
       "  'mold',\n",
       "  'mess',\n",
       "  'with',\n",
       "  'your',\n",
       "  'head',\n",
       "  'and',\n",
       "  'such',\n",
       "  'lost',\n",
       "  'highway',\n",
       "  'memento',\n",
       "  'but',\n",
       "  'there',\n",
       "  'are',\n",
       "  'good',\n",
       "  'and',\n",
       "  'bad',\n",
       "  'ways',\n",
       "  'of',\n",
       "  'making',\n",
       "  'all',\n",
       "  'types',\n",
       "  'of',\n",
       "  'films',\n",
       "  'and',\n",
       "  'these',\n",
       "  'folks',\n",
       "  'just',\n",
       "  'didn',\n",
       "  't',\n",
       "  'snag',\n",
       "  'this',\n",
       "  'one',\n",
       "  'correctly',\n",
       "  'they',\n",
       "  'seem',\n",
       "  'to',\n",
       "  'have',\n",
       "  'taken',\n",
       "  'this',\n",
       "  'pretty',\n",
       "  'neat',\n",
       "  'concept',\n",
       "  'but',\n",
       "  'executed',\n",
       "  'it',\n",
       "  'terribly',\n",
       "  'so',\n",
       "  'what',\n",
       "  'are',\n",
       "  'the',\n",
       "  'problems',\n",
       "  'with',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'well',\n",
       "  'its',\n",
       "  'main',\n",
       "  'problem',\n",
       "  'is',\n",
       "  'that',\n",
       "  'it',\n",
       "  's',\n",
       "  'simply',\n",
       "  'too',\n",
       "  'jumbled',\n",
       "  'it',\n",
       "  'starts',\n",
       "  'off',\n",
       "  'normal',\n",
       "  'but',\n",
       "  'then',\n",
       "  'downshifts',\n",
       "  'into',\n",
       "  'this',\n",
       "  'fantasy',\n",
       "  'world',\n",
       "  'in',\n",
       "  'which',\n",
       "  'you',\n",
       "  'as',\n",
       "  'an',\n",
       "  'audience',\n",
       "  'member',\n",
       "  'have',\n",
       "  'no',\n",
       "  'idea',\n",
       "  'what',\n",
       "  's',\n",
       "  'going',\n",
       "  'on',\n",
       "  'there',\n",
       "  'are',\n",
       "  'dreams',\n",
       "  'there',\n",
       "  'are',\n",
       "  'characters',\n",
       "  'coming',\n",
       "  'back',\n",
       "  'from',\n",
       "  'the',\n",
       "  'dead',\n",
       "  'there',\n",
       "  'are',\n",
       "  'others',\n",
       "  'who',\n",
       "  'look',\n",
       "  'like',\n",
       "  'the',\n",
       "  'dead',\n",
       "  'there',\n",
       "  'are',\n",
       "  'strange',\n",
       "  'apparitions',\n",
       "  'there',\n",
       "  'are',\n",
       "  'disappearances',\n",
       "  'there',\n",
       "  'are',\n",
       "  'a',\n",
       "  'looooot',\n",
       "  'of',\n",
       "  'chase',\n",
       "  'scenes',\n",
       "  'there',\n",
       "  'are',\n",
       "  'tons',\n",
       "  'of',\n",
       "  'weird',\n",
       "  'things',\n",
       "  'that',\n",
       "  'happen',\n",
       "  'and',\n",
       "  'most',\n",
       "  'of',\n",
       "  'it',\n",
       "  'is',\n",
       "  'simply',\n",
       "  'not',\n",
       "  'explained',\n",
       "  'now',\n",
       "  'i',\n",
       "  'personally',\n",
       "  'don',\n",
       "  't',\n",
       "  'mind',\n",
       "  'trying',\n",
       "  'to',\n",
       "  'unravel',\n",
       "  'a',\n",
       "  'film',\n",
       "  'every',\n",
       "  'now',\n",
       "  'and',\n",
       "  'then',\n",
       "  'but',\n",
       "  'when',\n",
       "  'all',\n",
       "  'it',\n",
       "  'does',\n",
       "  'is',\n",
       "  'give',\n",
       "  'me',\n",
       "  'the',\n",
       "  'same',\n",
       "  'clue',\n",
       "  'over',\n",
       "  'and',\n",
       "  'over',\n",
       "  'again',\n",
       "  'i',\n",
       "  'get',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'fed',\n",
       "  'up',\n",
       "  'after',\n",
       "  'a',\n",
       "  'while',\n",
       "  'which',\n",
       "  'is',\n",
       "  'this',\n",
       "  'film',\n",
       "  's',\n",
       "  'biggest',\n",
       "  'problem',\n",
       "  'it',\n",
       "  's',\n",
       "  'obviously',\n",
       "  'got',\n",
       "  'this',\n",
       "  'big',\n",
       "  'secret',\n",
       "  'to',\n",
       "  'hide',\n",
       "  'but',\n",
       "  'it',\n",
       "  'seems',\n",
       "  'to',\n",
       "  'want',\n",
       "  'to',\n",
       "  'hide',\n",
       "  'it',\n",
       "  'completely',\n",
       "  'until',\n",
       "  'its',\n",
       "  'final',\n",
       "  'five',\n",
       "  'minutes',\n",
       "  'and',\n",
       "  'do',\n",
       "  'they',\n",
       "  'make',\n",
       "  'things',\n",
       "  'entertaining',\n",
       "  'thrilling',\n",
       "  'or',\n",
       "  'even',\n",
       "  'engaging',\n",
       "  'in',\n",
       "  'the',\n",
       "  'meantime',\n",
       "  'not',\n",
       "  'really',\n",
       "  'the',\n",
       "  'sad',\n",
       "  'part',\n",
       "  'is',\n",
       "  'that',\n",
       "  'the',\n",
       "  'arrow',\n",
       "  'and',\n",
       "  'i',\n",
       "  'both',\n",
       "  'dig',\n",
       "  'on',\n",
       "  'flicks',\n",
       "  'like',\n",
       "  'this',\n",
       "  'so',\n",
       "  'we',\n",
       "  'actually',\n",
       "  'figured',\n",
       "  'most',\n",
       "  'of',\n",
       "  'it',\n",
       "  'out',\n",
       "  'by',\n",
       "  'the',\n",
       "  'half',\n",
       "  'way',\n",
       "  'point',\n",
       "  'so',\n",
       "  'all',\n",
       "  'of',\n",
       "  'the',\n",
       "  'strangeness',\n",
       "  'after',\n",
       "  'that',\n",
       "  'did',\n",
       "  'start',\n",
       "  'to',\n",
       "  'make',\n",
       "  'a',\n",
       "  'little',\n",
       "  'bit',\n",
       "  'of',\n",
       "  'sense',\n",
       "  'but',\n",
       "  'it',\n",
       "  'still',\n",
       "  'didn',\n",
       "  't',\n",
       "  'the',\n",
       "  'make',\n",
       "  'the',\n",
       "  'film',\n",
       "  'all',\n",
       "  'that',\n",
       "  'more',\n",
       "  'entertaining',\n",
       "  'i',\n",
       "  'guess',\n",
       "  'the',\n",
       "  'bottom',\n",
       "  'line',\n",
       "  'with',\n",
       "  'movies',\n",
       "  'like',\n",
       "  'this',\n",
       "  'is',\n",
       "  'that',\n",
       "  'you',\n",
       "  'should',\n",
       "  'always',\n",
       "  'make',\n",
       "  'sure',\n",
       "  'that',\n",
       "  'the',\n",
       "  'audience',\n",
       "  'is',\n",
       "  'into',\n",
       "  'it',\n",
       "  'even',\n",
       "  'before',\n",
       "  'they',\n",
       "  'are',\n",
       "  'given',\n",
       "  'the',\n",
       "  'secret',\n",
       "  'password',\n",
       "  'to',\n",
       "  'enter',\n",
       "  'your',\n",
       "  'world',\n",
       "  'of',\n",
       "  'understanding',\n",
       "  'i',\n",
       "  'mean',\n",
       "  'showing',\n",
       "  'melissa',\n",
       "  'sagemiller',\n",
       "  'running',\n",
       "  'away',\n",
       "  'from',\n",
       "  'visions',\n",
       "  'for',\n",
       "  'about',\n",
       "  '20',\n",
       "  'minutes',\n",
       "  'throughout',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'is',\n",
       "  'just',\n",
       "  'plain',\n",
       "  'lazy',\n",
       "  'okay',\n",
       "  'we',\n",
       "  'get',\n",
       "  'it',\n",
       "  'there',\n",
       "  'are',\n",
       "  'people',\n",
       "  'chasing',\n",
       "  'her',\n",
       "  'and',\n",
       "  'we',\n",
       "  'don',\n",
       "  't',\n",
       "  'know',\n",
       "  'who',\n",
       "  'they',\n",
       "  'are',\n",
       "  'do',\n",
       "  'we',\n",
       "  'really',\n",
       "  'need',\n",
       "  'to',\n",
       "  'see',\n",
       "  'it',\n",
       "  'over',\n",
       "  'and',\n",
       "  'over',\n",
       "  'again',\n",
       "  'how',\n",
       "  'about',\n",
       "  'giving',\n",
       "  'us',\n",
       "  'different',\n",
       "  'scenes',\n",
       "  'offering',\n",
       "  'further',\n",
       "  'insight',\n",
       "  'into',\n",
       "  'all',\n",
       "  'of',\n",
       "  'the',\n",
       "  'strangeness',\n",
       "  'going',\n",
       "  'down',\n",
       "  'in',\n",
       "  'the',\n",
       "  'movie',\n",
       "  'apparently',\n",
       "  'the',\n",
       "  'studio',\n",
       "  'took',\n",
       "  'this',\n",
       "  'film',\n",
       "  'away',\n",
       "  'from',\n",
       "  'its',\n",
       "  'director',\n",
       "  'and',\n",
       "  'chopped',\n",
       "  'it',\n",
       "  'up',\n",
       "  'themselves',\n",
       "  'and',\n",
       "  'it',\n",
       "  'shows',\n",
       "  'there',\n",
       "  'might',\n",
       "  've',\n",
       "  'been',\n",
       "  'a',\n",
       "  'pretty',\n",
       "  'decent',\n",
       "  'teen',\n",
       "  'mind',\n",
       "  'fuck',\n",
       "  'movie',\n",
       "  'in',\n",
       "  'here',\n",
       "  'somewhere',\n",
       "  'but',\n",
       "  'i',\n",
       "  'guess',\n",
       "  'the',\n",
       "  'suits',\n",
       "  'decided',\n",
       "  'that',\n",
       "  'turning',\n",
       "  'it',\n",
       "  'into',\n",
       "  'a',\n",
       "  'music',\n",
       "  'video',\n",
       "  'with',\n",
       "  'little',\n",
       "  'edge',\n",
       "  'would',\n",
       "  'make',\n",
       "  'more',\n",
       "  'sense',\n",
       "  'the',\n",
       "  'actors',\n",
       "  'are',\n",
       "  'pretty',\n",
       "  'good',\n",
       "  'for',\n",
       "  'the',\n",
       "  'most',\n",
       "  'part',\n",
       "  'although',\n",
       "  'wes',\n",
       "  'bentley',\n",
       "  'just',\n",
       "  'seemed',\n",
       "  'to',\n",
       "  'be',\n",
       "  'playing',\n",
       "  'the',\n",
       "  'exact',\n",
       "  'same',\n",
       "  'character',\n",
       "  'that',\n",
       "  'he',\n",
       "  'did',\n",
       "  'in',\n",
       "  'american',\n",
       "  'beauty',\n",
       "  'only',\n",
       "  'in',\n",
       "  'a',\n",
       "  'new',\n",
       "  'neighborhood',\n",
       "  'but',\n",
       "  'my',\n",
       "  'biggest',\n",
       "  'kudos',\n",
       "  'go',\n",
       "  'out',\n",
       "  'to',\n",
       "  'sagemiller',\n",
       "  'who',\n",
       "  'holds',\n",
       "  'her',\n",
       "  'own',\n",
       "  'throughout',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'film',\n",
       "  'and',\n",
       "  'actually',\n",
       "  'has',\n",
       "  'you',\n",
       "  'feeling',\n",
       "  'her',\n",
       "  'character',\n",
       "  's',\n",
       "  'unraveling',\n",
       "  'overall',\n",
       "  'the',\n",
       "  'film',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'stick',\n",
       "  'because',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'entertain',\n",
       "  'it',\n",
       "  's',\n",
       "  'confusing',\n",
       "  'it',\n",
       "  'rarely',\n",
       "  'excites',\n",
       "  'and',\n",
       "  'it',\n",
       "  'feels',\n",
       "  'pretty',\n",
       "  'redundant',\n",
       "  'for',\n",
       "  'most',\n",
       "  'of',\n",
       "  'its',\n",
       "  'runtime',\n",
       "  'despite',\n",
       "  'a',\n",
       "  'pretty',\n",
       "  'cool',\n",
       "  'ending',\n",
       "  'and',\n",
       "  'explanation',\n",
       "  'to',\n",
       "  'all',\n",
       "  'of',\n",
       "  'the',\n",
       "  'craziness',\n",
       "  'that',\n",
       "  'came',\n",
       "  'before',\n",
       "  'it',\n",
       "  'oh',\n",
       "  'and',\n",
       "  'by',\n",
       "  'the',\n",
       "  'way',\n",
       "  'this',\n",
       "  'is',\n",
       "  'not',\n",
       "  'a',\n",
       "  'horror',\n",
       "  'or',\n",
       "  'teen',\n",
       "  'slasher',\n",
       "  'flick',\n",
       "  'it',\n",
       "  's',\n",
       "  'just',\n",
       "  'packaged',\n",
       "  'to',\n",
       "  'look',\n",
       "  'that',\n",
       "  'way',\n",
       "  'because',\n",
       "  'someone',\n",
       "  'is',\n",
       "  'apparently',\n",
       "  'assuming',\n",
       "  'that',\n",
       "  'the',\n",
       "  'genre',\n",
       "  'is',\n",
       "  'still',\n",
       "  'hot',\n",
       "  'with',\n",
       "  'the',\n",
       "  'kids',\n",
       "  'it',\n",
       "  'also',\n",
       "  'wrapped',\n",
       "  'production',\n",
       "  'two',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'and',\n",
       "  'has',\n",
       "  'been',\n",
       "  'sitting',\n",
       "  'on',\n",
       "  'the',\n",
       "  'shelves',\n",
       "  'ever',\n",
       "  'since',\n",
       "  'whatever',\n",
       "  'skip',\n",
       "  'it',\n",
       "  'where',\n",
       "  's',\n",
       "  'joblo',\n",
       "  'coming',\n",
       "  'from',\n",
       "  'a',\n",
       "  'nightmare',\n",
       "  'of',\n",
       "  'elm',\n",
       "  'street',\n",
       "  '3',\n",
       "  '7',\n",
       "  '10',\n",
       "  'blair',\n",
       "  'witch',\n",
       "  '2',\n",
       "  '7',\n",
       "  '10',\n",
       "  'the',\n",
       "  'crow',\n",
       "  '9',\n",
       "  '10',\n",
       "  'the',\n",
       "  'crow',\n",
       "  'salvation',\n",
       "  '4',\n",
       "  '10',\n",
       "  'lost',\n",
       "  'highway',\n",
       "  '10',\n",
       "  '10',\n",
       "  'memento',\n",
       "  '10',\n",
       "  '10',\n",
       "  'the',\n",
       "  'others',\n",
       "  '9',\n",
       "  '10',\n",
       "  'stir',\n",
       "  'of',\n",
       "  'echoes',\n",
       "  '8',\n",
       "  '10'],\n",
       " ['the',\n",
       "  'happy',\n",
       "  'bastard',\n",
       "  's',\n",
       "  'quick',\n",
       "  'movie',\n",
       "  'review',\n",
       "  'damn',\n",
       "  'that',\n",
       "  'y2k',\n",
       "  'bug',\n",
       "  'it',\n",
       "  's',\n",
       "  'got',\n",
       "  'a',\n",
       "  'head',\n",
       "  'start',\n",
       "  'in',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'starring',\n",
       "  'jamie',\n",
       "  'lee',\n",
       "  'curtis',\n",
       "  'and',\n",
       "  'another',\n",
       "  'baldwin',\n",
       "  'brother',\n",
       "  'william',\n",
       "  'this',\n",
       "  'time',\n",
       "  'in',\n",
       "  'a',\n",
       "  'story',\n",
       "  'regarding',\n",
       "  'a',\n",
       "  'crew',\n",
       "  'of',\n",
       "  'a',\n",
       "  'tugboat',\n",
       "  'that',\n",
       "  'comes',\n",
       "  'across',\n",
       "  'a',\n",
       "  'deserted',\n",
       "  'russian',\n",
       "  'tech',\n",
       "  'ship',\n",
       "  'that',\n",
       "  'has',\n",
       "  'a',\n",
       "  'strangeness',\n",
       "  'to',\n",
       "  'it',\n",
       "  'when',\n",
       "  'they',\n",
       "  'kick',\n",
       "  'the',\n",
       "  'power',\n",
       "  'back',\n",
       "  'on',\n",
       "  'little',\n",
       "  'do',\n",
       "  'they',\n",
       "  'know',\n",
       "  'the',\n",
       "  'power',\n",
       "  'within',\n",
       "  'going',\n",
       "  'for',\n",
       "  'the',\n",
       "  'gore',\n",
       "  'and',\n",
       "  'bringing',\n",
       "  'on',\n",
       "  'a',\n",
       "  'few',\n",
       "  'action',\n",
       "  'sequences',\n",
       "  'here',\n",
       "  'and',\n",
       "  'there',\n",
       "  'virus',\n",
       "  'still',\n",
       "  'feels',\n",
       "  'very',\n",
       "  'empty',\n",
       "  'like',\n",
       "  'a',\n",
       "  'movie',\n",
       "  'going',\n",
       "  'for',\n",
       "  'all',\n",
       "  'flash',\n",
       "  'and',\n",
       "  'no',\n",
       "  'substance',\n",
       "  'we',\n",
       "  'don',\n",
       "  't',\n",
       "  'know',\n",
       "  'why',\n",
       "  'the',\n",
       "  'crew',\n",
       "  'was',\n",
       "  'really',\n",
       "  'out',\n",
       "  'in',\n",
       "  'the',\n",
       "  'middle',\n",
       "  'of',\n",
       "  'nowhere',\n",
       "  'we',\n",
       "  'don',\n",
       "  't',\n",
       "  'know',\n",
       "  'the',\n",
       "  'origin',\n",
       "  'of',\n",
       "  'what',\n",
       "  'took',\n",
       "  'over',\n",
       "  'the',\n",
       "  'ship',\n",
       "  'just',\n",
       "  'that',\n",
       "  'a',\n",
       "  'big',\n",
       "  'pink',\n",
       "  'flashy',\n",
       "  'thing',\n",
       "  'hit',\n",
       "  'the',\n",
       "  'mir',\n",
       "  'and',\n",
       "  'of',\n",
       "  'course',\n",
       "  'we',\n",
       "  'don',\n",
       "  't',\n",
       "  'know',\n",
       "  'why',\n",
       "  'donald',\n",
       "  'sutherland',\n",
       "  'is',\n",
       "  'stumbling',\n",
       "  'around',\n",
       "  'drunkenly',\n",
       "  'throughout',\n",
       "  'here',\n",
       "  'it',\n",
       "  's',\n",
       "  'just',\n",
       "  'hey',\n",
       "  'let',\n",
       "  's',\n",
       "  'chase',\n",
       "  'these',\n",
       "  'people',\n",
       "  'around',\n",
       "  'with',\n",
       "  'some',\n",
       "  'robots',\n",
       "  'the',\n",
       "  'acting',\n",
       "  'is',\n",
       "  'below',\n",
       "  'average',\n",
       "  'even',\n",
       "  'from',\n",
       "  'the',\n",
       "  'likes',\n",
       "  'of',\n",
       "  'curtis',\n",
       "  'you',\n",
       "  're',\n",
       "  'more',\n",
       "  'likely',\n",
       "  'to',\n",
       "  'get',\n",
       "  'a',\n",
       "  'kick',\n",
       "  'out',\n",
       "  'of',\n",
       "  'her',\n",
       "  'work',\n",
       "  'in',\n",
       "  'halloween',\n",
       "  'h20',\n",
       "  'sutherland',\n",
       "  'is',\n",
       "  'wasted',\n",
       "  'and',\n",
       "  'baldwin',\n",
       "  'well',\n",
       "  'he',\n",
       "  's',\n",
       "  'acting',\n",
       "  'like',\n",
       "  'a',\n",
       "  'baldwin',\n",
       "  'of',\n",
       "  'course',\n",
       "  'the',\n",
       "  'real',\n",
       "  'star',\n",
       "  'here',\n",
       "  'are',\n",
       "  'stan',\n",
       "  'winston',\n",
       "  's',\n",
       "  'robot',\n",
       "  'design',\n",
       "  'some',\n",
       "  'schnazzy',\n",
       "  'cgi',\n",
       "  'and',\n",
       "  'the',\n",
       "  'occasional',\n",
       "  'good',\n",
       "  'gore',\n",
       "  'shot',\n",
       "  'like',\n",
       "  'picking',\n",
       "  'into',\n",
       "  'someone',\n",
       "  's',\n",
       "  'brain',\n",
       "  'so',\n",
       "  'if',\n",
       "  'robots',\n",
       "  'and',\n",
       "  'body',\n",
       "  'parts',\n",
       "  'really',\n",
       "  'turn',\n",
       "  'you',\n",
       "  'on',\n",
       "  'here',\n",
       "  's',\n",
       "  'your',\n",
       "  'movie',\n",
       "  'otherwise',\n",
       "  'it',\n",
       "  's',\n",
       "  'pretty',\n",
       "  'much',\n",
       "  'a',\n",
       "  'sunken',\n",
       "  'ship',\n",
       "  'of',\n",
       "  'a',\n",
       "  'movie']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# tokenize one sentence\n",
    "x1_tokens = tokenizer.tokenize(X[1])\n",
    "\n",
    "len(x1_tokens)\n",
    "x1_tokens[0:11]\n",
    "\n",
    "# return maps \n",
    "tokens = map(lambda s: tokenizer.tokenize(s),X)\n",
    "\n",
    "tokens_list = list(tokens)\n",
    "\n",
    "# convert to list of list\n",
    "len(tokens_list)\n",
    "tokens_list[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create text\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data in nlp\n",
    "\n",
    "refs: https://chrisalbon.com/#articles\n",
    "\n",
    "* remove punctuation\n",
    "* remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = ['Hi!!!! I. Love. This. Song....', \n",
    "             '10000% Agree!!!! #LoveIT', \n",
    "             'Right?!?!']\n",
    "\n",
    "# Apply function\n",
    "[tut.remove_punctuation(sentence) for sentence in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 124 words\n"
     ]
    }
   ],
   "source": [
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Remove stop words\n",
    "x1_tokens_1 = [word for word in x1_tokens if word not in stop_words]\n",
    "len(x1_tokens_1)\n",
    "\n",
    "print('Removed {} words'.format(len(x1_tokens) -len(x1_tokens_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet', 'tradit']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create word tokens\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting', 'tradition']\n",
    "\n",
    "# Create stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Apply stemmer\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "refs: \n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting', 'tradition']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: Review\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemma\n",
    "[lemma.lemmatize(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "```\n",
    "\n",
    "| Tag | Part Of Speech                     |\n",
    "|---- | -----------------------------------|\n",
    "| NNP | Proper noun, singular              |\n",
    "| NN  | Noun, singular or mass             |\n",
    "| RB  | Adverb                             |\n",
    "| VBD | Verb, past tense                   |\n",
    "| VBG | Verb, gerund or present participle |\n",
    "| JJ  | Adjective                          |\n",
    "| PRP | Personal pronoun                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# Use pre-trained part of speech tagger\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "# Show parts of speech\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization\n",
    "\n",
    "The main idea of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT \n",
    "\n",
    "TODO: Move this scetion to machine_learnin repo\n",
    "\n",
    "refs: http://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "TODO: ADD Code to do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "1. BERT Google release in 2018 and beat all the previous model   <= #A very important\n",
    "1. use wrodvec\n",
    "1. glove\n",
    "\n",
    "for help with BERT\n",
    "https://blog.insightdatascience.com/using-bert-for-state-of-the-art-pre-training-for-natural-language-processing-1d87142c29e7\n",
    "\n",
    "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "clone bert\n",
    "git clone https://github.com/google-research/bert.git\n",
    "\n",
    "\n",
    "Use kaglle competition (Good example of multi-label calssifier)\n",
    "https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n",
    "\n",
    "Yuo can get the data from kaggle\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BERT uses the idea of Transformers: Encoder + ~~Decoder~~ does not need decoder\n",
    "* BERT does not work durectional like RNN. Process from left to right. The transformer reads the entore sentence at once. Because of that, It has the hability to learn the context of a word based on the entire surround.\n",
    "\n",
    "* Many models predict the next word in a sequence (e.g. “The child came home from ___”), a directional approach which inherently limits context learning. To overcome this challenge, BERT uses two training strategies:\n",
    "\n",
    "    1. Masked LM (MLM)\n",
    "    1. Next Sentence Prediction (NSP)\n",
    "\n",
    "----------------------\n",
    "**MLM**\n",
    "\n",
    "* NN is feed if sentences. In a sentence some words are labels as ```<MASK>``` and the goal of the NN is learn how to predict <MASK> based on the context of the other words in the sentence. **Word2Vec** works similar but with **skipgrams**\n",
    "\n",
    "* The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, \n",
    "    \n",
    "\n",
    "------\n",
    "    \n",
    "**Aplication**\n",
    "\n",
    "1. Sentiment Analysis: \n",
    "    * input: Movie/Product review. Output: is the review positive or negative?\n",
    "    * Example dataset: [SST](https://nlp.stanford.edu/sentiment/)\n",
    "    \n",
    "    \n",
    "\n",
    "<img src=\"images/BERT-classification-spam.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "1. NER: Name Entity Recognition (is a Person, Organization, Date, etc)\n",
    "    \n",
    "\n",
    "1. Classifications adding one more layer or feed a new classifier with the embedding as input\n",
    "    \n",
    "    \n",
    "1. **Fact checking**:\n",
    "    * Input: sentence. Output: “Claim” or “Not Claim”\n",
    "    * More ambitious/futuristic example:\n",
    "    * Input: Claim sentence. Output: “True” or “False”\n",
    "    * Full Fact is an organization building automatic fact-checking tools for the benefit of the public. Part of their pipeline is a classifier that reads news articles and detects claims (classifies text as either “claim” or “not claim”) which can later be fact-checked (by humans now, with ML later, hopefully).\n",
    "Video: Sentence embeddings for automated factchecking - Lev Konstantinovskiy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "    \n",
    "**Take away** \n",
    "    \n",
    "    \n",
    "* **Model size matters**, even at huge scale. BERT_large, with 345 million parameters, is the largest model of its kind. It is demonstrably superior on small-scale tasks to BERT_base, which uses the same architecture with “only” 110 million parameters.\n",
    "    \n",
    "    \n",
    "* **With enough training data**, more training steps == higher accuracy. For instance, on the MNLI task, the BERT_base accuracy improves by 1.0% when trained on 1M steps (128,000 words batch size) compared to 500K steps with the same batch size.\n",
    "    \n",
    "* **BERT’s bidirectional approach (MLM) converges slower** than left-to-right approaches (because only 15% of words are predicted in each batch) but bidirectional training still outperforms left-to-right training after a small number of pre-training steps.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

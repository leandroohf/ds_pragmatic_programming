{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da8d358",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "refs:\n",
    "\n",
    "* https://docs.scrapy.org/en/latest/topics/shell.html\n",
    "* https://towardsdatascience.com/a-minimalist-end-to-end-scrapy-tutorial-part-i-11e350bcdec0\n",
    "    * `git clone https://github.com/harrywang/scrapy-tutorial-starter.git`\n",
    "* https://towardsdatascience.com/scrapy-this-is-how-to-successfully-login-with-ease-ea980e2c5901\n",
    "* https://github.com/leandroohf/scrapping-tutorial  <=== **MY REPO**\n",
    "    * `git clone https://github.com/leandroohf/scrapping-tutorial`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91fc6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a983b97",
   "metadata": {},
   "source": [
    "## Archictecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3fd4b",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/scrapy_architecture.png\" style=\"float:left\" width=\"800\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8904af",
   "metadata": {},
   "source": [
    "## Scrap shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b8564",
   "metadata": {},
   "source": [
    "* It is also a regular iPython shell\n",
    "* Use for testing XPath or CSS expressions and what data they extracting (DEBUG)\n",
    "* Web site example:\n",
    "    * http://quotes.toscrape.com/\n",
    "    * https://www.beerwulf.com/en-gb/c/mixedbeercases  <===\n",
    "\n",
    "\n",
    "```shell\n",
    "# scrapy shell <url>\n",
    "scrapy shell http://quotes.toscrape.com/\n",
    "\n",
    "# file examples\n",
    "# UNIX-style\n",
    "scrapy shell ./path/to/file.html\n",
    "\n",
    "# File URI\n",
    "scrapy shell file:///absolute/path/to/file.html\n",
    "\n",
    "```\n",
    "\n",
    "How to use: `scrapy shell`:\n",
    "\n",
    "Run inside the shell \n",
    "\n",
    "```python\n",
    "\n",
    "# inside scrapy shell  <======\n",
    "# inspecting settings object\n",
    "seettings\n",
    "\n",
    "# fetch the page\n",
    "fetch('https://www.beerwulf.com/en-gb/c/mixedbeercases')\n",
    "\n",
    "# check response object\n",
    "response\n",
    "\n",
    "response.status\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(response.headers)\n",
    "\n",
    "# > Out[2]: <200 https://www.beerwulf.com/en-gb/c/mixedbeercases>\n",
    "\n",
    "# inspect html code \n",
    "response.body\n",
    "response.body_as_unicode()\n",
    "\n",
    "# extract title using css xpath \n",
    "response.css('title::text').get()  # get the first results\n",
    "#> Out[12]: 'Mixed Beer Cases  | Discover our beers | Beerwulf'\n",
    "\n",
    "# get all beers\n",
    "response.css('h4::text').getall()  \n",
    "\n",
    "# Out[13]:\n",
    "#[' Search results',\n",
    "# 'THE SUB  (2L)',\n",
    "# 'BLADE  (8L)',\n",
    "# 'Beer Tap Starter Packs',\n",
    "# 'All Beer Taps',\n",
    "# 'SUB Kegs',\n",
    "# ...]\n",
    "\n",
    "# inspect the object crawler\n",
    "crawler.stats.get_stats()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a9367",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Project folder struture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7d21c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Create project\n",
    "\n",
    "\n",
    "```shell\n",
    "scrapy startproject tutorial # project-name \n",
    "```\n",
    "\n",
    "Folder explained:\n",
    "\n",
    "* scrapy.cfg: the project configuration file\n",
    "* tutorial/: the project’s python module, you’ll later import your code from here.\n",
    "* tutorial/items.py: the project’s items file.\n",
    "* tutorial/pipelines.py: the project’s pipelines file.\n",
    "* tutorial/settings.py: the project’s settings file.\n",
    "* tutorial/spiders/: a directory where you’ll later put your spiders.\n",
    "\n",
    "\n",
    "<img src=\"images/scrapy_project_folder_struture.png\" style=\"float:left\" width=\"300\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146b07b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Run crawler\n",
    "\n",
    "\n",
    "```shell\n",
    "# run spider\n",
    " scrapy crawl quotes\n",
    "\n",
    "# save output in json \n",
    "scrapy crawl quotes -o quotes.json\n",
    "```\n",
    "\n",
    "\n",
    "### Access settings\n",
    "\n",
    "```pyton\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        print(f\"Existing settings: {self.settings.attributes.keys()}\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe0505",
   "metadata": {},
   "source": [
    "## Xpath and css selectors\n",
    "\n",
    "* https://www.w3schools.com/xml/xpath_syntax.asp\n",
    "* xpath chest sheet: https://devhints.io/xpath\n",
    "* https://www.w3schools.com/cssref/css_selectors.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7f26b",
   "metadata": {},
   "source": [
    "\n",
    "HTML example\n",
    "\n",
    "```html\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "\n",
    "<bookstore>\n",
    "\n",
    "<book>\n",
    "  <title lang=\"en\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "\n",
    "<book>\n",
    "  <title lang=\"en\">Learning XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "\n",
    "</bookstore>\n",
    "```\n",
    "\n",
    "* Selecting nodes\n",
    "    * bookstore: select all nodes called bookstore\n",
    "    * /bookstore: start from root\n",
    "    * bookstore/book: Selects all book elements that are children of bookstore\n",
    "    * //book: Selects all book elements no matter where they are in the document\n",
    "    * bookstore//book: select all book that is child of bookstore no matter where they are under bookstore umbrela\n",
    "\n",
    "* Predicate (positioning)\n",
    "\n",
    "    * /bookstore/book[1]: select first\n",
    "    * /bookstore/book[last()]\n",
    "    * /bookstore/book[position()<3]\n",
    "    * /bookstore/*: select all child\n",
    "    \n",
    "* Select multiple paths\n",
    "\n",
    "    * //book/title | //book/price: title AND proce\n",
    "    * //title | //price: title OR price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285ecea",
   "metadata": {},
   "source": [
    "* common tasks (xpath or css)\n",
    "\n",
    "    * get href link\n",
    "    * get text of node\n",
    "    * get image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1693dcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['More Info', 'click here', 'Basketball ', 'Data Scientis ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'http://example.com'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'http://example.com/img.jpg'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Basketball ', 'Data Scientis ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Basketball ', 'Data Scientis ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Basketball ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is good for test scrapy code\n",
    "from scrapy import Selector\n",
    "\n",
    "text = \"\"\"\n",
    "<a href = \"http://example.com\"\">More Info<strong>click here</strong></a>\n",
    "<img src=\"http://example.com/img.jpg\" class='photo-large' />\n",
    "<a name='Sport' class='a-size'>Basketball </a>\n",
    "<a name='Proffession' class='a-size'>Data Scientis </a>\n",
    "\"\"\"\n",
    "\n",
    "val = Selector(text = text)\n",
    "\n",
    "# text\n",
    "val.xpath('//a//text()').getall()\n",
    "\n",
    "# link\n",
    "val.xpath('//a/@href').get()\n",
    "\n",
    "# image \n",
    "val.xpath('//img/@src').get()\n",
    "\n",
    "\n",
    "# filter \n",
    "val.xpath(\"//a[@class='a-size']/text()\").getall()\n",
    "val.xpath('//a[has-class(\"a-size\")]/text()').getall()\n",
    "\n",
    "# filter multiple criteria\n",
    "val.xpath(\"//a[@name='Sport' and @class='a-size']/text()\").getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64148435",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ItemLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9744451",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "This make easy to save to json or database. \n",
    "\n",
    "Scrapy supports the following types of items, via the itemadapter library: dictionaries, Item objects, dataclass objects, \n",
    "\n",
    "Steps:\n",
    "\n",
    "1. define QuoteItem class in item.py file\n",
    "1. define the preprocessing functions in item.py \n",
    "1. \n",
    "\n",
    "\n",
    "Ex: item.py\n",
    "```python\n",
    "# file: item.py\n",
    " Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "from scrapy.loader.processors import MapCompose, TakeFirst\n",
    "from scrapy.item import Item, Field\n",
    "from datetime import datetime\n",
    "\n",
    "## =============== Preprocessing functions to treat fields =============== \n",
    "# remove the Unicode quotation\n",
    "def remove_quotes(text):\n",
    "    # strip the unicode quotes\n",
    "    text = text.strip(u'\\u201c'u'\\u201d')\n",
    "    return text\n",
    "\n",
    "def convert_date(text):\n",
    "    # convert string March 14, 1879 to Python date\n",
    "    return datetime.strptime(text, '%B %d, %Y')\n",
    "\n",
    "\n",
    "def parse_location(text):\n",
    "    # parse location \"in Ulm, Germany\"\n",
    "    # this simply remove \"in \", you can further parse city, state, country, etc.\n",
    "    return text[3:]\n",
    "\n",
    "# Ex: scrapy.Item\n",
    "class QuoteItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    " \n",
    "    quote_content = Field(\n",
    "        input_processor=MapCompose(remove_quotes),\n",
    "        # TakeFirst return the first value not the whole list\n",
    "        output_processor=TakeFirst()\n",
    "        )\n",
    "\n",
    "    author_name = Field(\n",
    "        input_processor=MapCompose(str.strip),\n",
    "        output_processor=TakeFirst()\n",
    "        )\n",
    "    author_birthday = Field(\n",
    "        input_processor=MapCompose(convert_date),\n",
    "        output_processor=TakeFirst()\n",
    "    )\n",
    "    author_bornlocation = Field(\n",
    "        input_processor=MapCompose(parse_location),\n",
    "        output_processor=TakeFirst()\n",
    "    )\n",
    "    author_bio = Field(\n",
    "        input_processor=MapCompose(str.strip),\n",
    "        output_processor=TakeFirst()\n",
    "        )\n",
    "\n",
    "    tags = Field()\n",
    "    \n",
    "#Ex: dataclass. Has some beneficial \n",
    "from typing import List\n",
    "from dataclasses import dataclass, field\n",
    "from dataclass_type_validator import dataclass_validate\n",
    "\n",
    "@dataclass_validate()\n",
    "@dataclass()\n",
    "class ProductItem():\n",
    "    public_name: str\n",
    "    product_type: str\n",
    "    sales_page: str\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e844968",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To load data in the pipeline you need to:\n",
    "    \n",
    "```python\n",
    "from tutorial.items import QuoteItem # Load the item definition you wrote in item.py\n",
    "from scrapy.loader import ItemLoader # Load the ItemLoader \n",
    "\n",
    "## in the middle of the code\n",
    "\n",
    "\n",
    "def parse_author(self, response):\n",
    "\n",
    "    # yield {\n",
    "    #     'author_name': response.css('.author-title::text').get(),\n",
    "    #     'author_birthday': response.css('.author-born-date::text').get(),\n",
    "    #     'author_bornlocation': response.css('.author-born-location::text').get(),\n",
    "    #     'author_bio': response.css('.author-description::text').get(),\n",
    "    # }\n",
    "\n",
    "    quote_item = response.meta['quote_item']\n",
    "\n",
    "    loader = ItemLoader(item=quote_item, response=response)\n",
    "    loader.add_css('author_name', '.author-title::text')\n",
    "    loader.add_css('author_birthday', '.author-born-date::text')\n",
    "    loader.add_css('author_bornlocation', '.author-born-location::text')\n",
    "    loader.add_css('author_bio', '.author-description::text')\n",
    "    \n",
    "    yield loader.load_item()\n",
    "\n",
    "```\n",
    "    \n",
    "Data class example\n",
    "\n",
    "https://stackoverflow.com/questions/67360406/is-there-any-example-about-how-to-use-dataclass-and-scrapy-items\n",
    "```python\n",
    "def parse(self, response):\n",
    "    \n",
    "    # code to extract the fields\n",
    "    \n",
    "     product = ProductItem(\n",
    "            public_name=public_name,\n",
    "            product_type=product_type,\n",
    "            sales_page=sales_page,\n",
    "            tags=tags\n",
    "             )\n",
    "        \n",
    "    # It is important to export to dict so you can use in the pipeline with an adapter  \n",
    "    yield { \"product\": product.__dict__ }\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6b1c8",
   "metadata": {},
   "source": [
    "## Save item in database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a5438",
   "metadata": {},
   "source": [
    "Steps:\n",
    "    \n",
    "1. Edit settings.py \n",
    "    1. Define how to connect on db insettings.py\n",
    "    1. Add itepipeline in settings.py\n",
    "1. model.py\n",
    "    1. ORM: create a data model (the tables on db)\n",
    "    1. develop a pipeline to save the items to a database. \n",
    "\n",
    "Pipleines:\n",
    "* Each item returned by the spider is sent to Item Pipelines (See architecture).\n",
    "* You define pipelines that are enables in settings.py\n",
    "* Create the new pipeline to save in db by editing the file: pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6cd18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "981b1e0b",
   "metadata": {},
   "source": [
    "**settings.py**\n",
    "```python\n",
    "\n",
    "## =========== db connections\n",
    "# Ex: sqlite\n",
    "CONNECTION_STRING = 'sqlite:///scrapy_quotes.db'\n",
    "\n",
    "# Exmaple: MySQL\n",
    "CONNECTION_STRING = \"{drivername}://{user}:{passwd}@{host}:{port}/{db_name}?charset=utf8\".format(\n",
    "     drivername=\"mysql\",\n",
    "     user=\"harrywang\",\n",
    "     passwd=\"tutorial\",\n",
    "     host=\"localhost\",\n",
    "     port=\"3306\",\n",
    "     db_name=\"scrapy_quotes\",\n",
    ")\n",
    "\n",
    "\n",
    "# ================ define new pipeline \n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "   #'tutorial.pipelines.TutorialPipeline': PRIORITY,  LOW priority execute first\n",
    "   'tutorial.pipelines.DuplicatesPipeline': 200,  # this run FIRST\n",
    "   'tutorial.pipelines.SaveQuotesPipeline': 300,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e696b",
   "metadata": {},
   "source": [
    "**model.py**\n",
    "\n",
    "```python\n",
    "from sqlalchemy import create_engine, Column, Table, ForeignKey, MetaData\n",
    "from sqlalchemy.orm import relationship\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import (\n",
    "    Integer, String, Date, DateTime, Float, Boolean, Text)\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\"\n",
    "    Performs database connection using database settings from settings.py.\n",
    "    Returns sqlalchemy engine instance\n",
    "    \"\"\"\n",
    "    return create_engine(get_project_settings().get(\"CONNECTION_STRING\"))\n",
    "\n",
    "\n",
    "def create_table(engine):\n",
    "    Base.metadata.create_all(engine)\n",
    "\n",
    "\n",
    "# Association Table for Many-to-Many relationship between Quote and Tag\n",
    "# https://docs.sqlalchemy.org/en/13/orm/basic_relationships.html#many-to-many\n",
    "quote_tag = Table('quote_tag', Base.metadata,\n",
    "    Column('quote_id', Integer, ForeignKey('quote.id')),\n",
    "    Column('tag_id', Integer, ForeignKey('tag.id'))\n",
    ")\n",
    "\n",
    "\n",
    "class Quote(Base):  # define table Quote\n",
    "    __tablename__ = \"quote\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    quote_content = Column('quote_content', Text())\n",
    "    author_id = Column(Integer, ForeignKey('author.id'))  # Many quotes to one author\n",
    "    tags = relationship('Tag', secondary='quote_tag',\n",
    "        lazy='dynamic', backref=\"quote\")  # M-to-M for quote and tag\n",
    "\n",
    "\n",
    "class Author(Base): # define table Author\n",
    "    __tablename__ = \"author\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column('name', String(50), unique=True)\n",
    "    birthday = Column('birthday', DateTime)\n",
    "    bornlocation = Column('bornlocation', String(150))\n",
    "    bio = Column('bio', Text())\n",
    "    quotes = relationship('Quote', backref='author')  # One author to many Quotes\n",
    "\n",
    "\n",
    "class Tag(Base): # define table Tag\n",
    "    __tablename__ = \"tag\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column('name', String(30), unique=True)\n",
    "    quotes = relationship('Quote', secondary='quote_tag',\n",
    "        lazy='dynamic', backref=\"tag\")  # M-to-M for quote and tag\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94662157",
   "metadata": {},
   "source": [
    "**pipelines.py**\n",
    "```python\n",
    "class SaveQuotesPipeline(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes database connection and sessionmaker\n",
    "        Creates tables\n",
    "        \"\"\"\n",
    "        engine = db_connect()\n",
    "        create_table(engine)\n",
    "        self.Session = sessionmaker(bind=engine)\n",
    "\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"Save quotes in the database\n",
    "        This method is called for every item pipeline component\n",
    "        \"\"\"\n",
    "        session = self.Session()\n",
    "        quote = Quote()\n",
    "        author = Author()\n",
    "        tag = Tag()\n",
    "        author.name = item[\"author_name\"]\n",
    "        author.birthday = item[\"author_birthday\"]\n",
    "        author.bornlocation = item[\"author_bornlocation\"]\n",
    "        author.bio = item[\"author_bio\"]\n",
    "        quote.quote_content = item[\"quote_content\"]\n",
    "\n",
    "        # check whether the author exists\n",
    "        exist_author = session.query(Author).filter_by(name = author.name).first()\n",
    "        if exist_author is not None:  # the current author exists\n",
    "            quote.author = exist_author\n",
    "        else:\n",
    "            quote.author = author\n",
    "\n",
    "        # check whether the current quote has tags or not\n",
    "        if \"tags\" in item:\n",
    "            for tag_name in item[\"tags\"]:\n",
    "                tag = Tag(name=tag_name)\n",
    "                # check whether the current tag already exists in the database\n",
    "                exist_tag = session.query(Tag).filter_by(name = tag.name).first()\n",
    "                if exist_tag is not None:  # the current tag exists\n",
    "                    tag = exist_tag\n",
    "                quote.tags.append(tag)\n",
    "\n",
    "        try:\n",
    "            session.add(quote)\n",
    "            session.commit()\n",
    "\n",
    "        except:\n",
    "            session.rollback()\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "        return item\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65245e0d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dealing with login and credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa6aa7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "refs:\n",
    "\n",
    "* https://quotes.toscrape.com/login\n",
    "* https://www.youtube.com/watch?v=I_vAGDZeg5Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9d952",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Steps:\n",
    "* Inspecting the page login\n",
    "\n",
    "    1. Do one time the login and under the tab Network discovery the token variable name under **FormData**\n",
    "    \n",
    "        * Look the variable: **csrf_token: DLIyfMtmuZjQJHSWCdhlsKiBPozwVbvREOqxFeUnNrTYAXGakpgc**\n",
    "\n",
    "\n",
    "<img src=\"images/inspecting_login_network_tab.png\" style=\"float:left\" width=\"1000\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694136dd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "* Inspecting the page login under tab Elements \n",
    "    * find the metadata for the token value\n",
    "\n",
    "```html\n",
    "<form action=\"/login\" method=\"post\" accept-charset=\"utf-8\">\n",
    "        <input type=\"hidden\" name=\"csrf_token\" value=\"DLIyfMtmuZjQJHSWCdhlsKiBPozwVbvREOqxFeUnNrTYAXGakpgc\">\n",
    "        <div class=\"row\">\n",
    "            <div class=\"form-group col-xs-3\">\n",
    "                <label for=\"username\">Username</label>\n",
    "                <input type=\"text\" class=\"form-control\" id=\"username\" name=\"username\">\n",
    "            </div>\n",
    "        </div>\n",
    "        <div class=\"row\">\n",
    "            <div class=\"form-group col-xs-3\">\n",
    "                <label for=\"username\">Password</label>\n",
    "                <input type=\"password\" class=\"form-control\" id=\"password\" name=\"password\">\n",
    "            </div>\n",
    "        </div>\n",
    "        <input type=\"submit\" value=\"Login\" class=\"btn btn-primary\">\n",
    "        \n",
    "    </form>\n",
    "```\n",
    "\n",
    "<img src=\"images/scrap_with_login_page.png\" style=\"float:left\" width=\"1000\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268a864",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Use `FormRequest` from scrapy.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from tutorial.items import QuoteItem\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'quotes-login'\n",
    "\n",
    "    start_urls = ['http://quotes.toscrape.com/login']\n",
    "\n",
    "    def start_scrap(self,response):\n",
    "\n",
    "        self.logger.info('========== Start scrapping =========== ')\n",
    "               \n",
    "        if response.status != 200:\n",
    "            \n",
    "            self.logger.error(\"Login failed!\")\n",
    "            \n",
    "            return \n",
    "        \n",
    "        quotes = response.css(\"div.quote\")\n",
    "\n",
    "   \n",
    "        quote_item = QuoteItem()\n",
    "        \n",
    "    \n",
    "        for quote in quotes:\n",
    "\n",
    "            text = quote.css('.text::text').get()\n",
    "            author = quote.css('.author::text').get()\n",
    "            tags = quote.css(\".tag::text\").getall()\n",
    "\n",
    "            loader = ItemLoader(item=QuoteItem(), selector=quote)\n",
    "        \n",
    "            loader.add_css('quote_content', '.text::text')\n",
    "            loader.add_css('tags', '.tag::text')\n",
    "            quote_item = loader.load_item()\n",
    "        \n",
    "           \n",
    "            self.logger.info(f'text: {text}')\n",
    "            self.logger.info(f'author: {author}')\n",
    "            self.logger.info(f'tags: {tags}')\n",
    "\n",
    "            self.logger.debug(\"-------------------------\")\n",
    "\n",
    "    def parse(self, response):\n",
    "    \n",
    "        # get the token value (the token expiration shoud define the end of section I guess)\n",
    "        token = response.css('form input::attr(value)').get()\n",
    "        \n",
    "        self.logger.info(f\"token: {token}\")\n",
    "\n",
    "        return FormRequest.from_response(response,formdata={\n",
    "            'csrf_token': token, \n",
    "            'username': 'leandro@gmail.com',\n",
    "            'password': 'dadisgood'\n",
    "        }, callback=self.start_scrap)\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702a59d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to ignore robots.txt for Scrapy spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ac658",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Website owners tell web spiders such as Googlebot what can and can't be crawled on their websites with the use of robots.txt file. The file resides on the root directory of a website and contain contain rules such as the followings;\n",
    "\n",
    "Steps for ignoring:\n",
    "\n",
    "\n",
    "1. set ignore robottxt rules in CLI\n",
    "\n",
    "```sh\n",
    "\n",
    "# when calling crawler\n",
    "scrapy crawl --set=ROBOTSTXT_OBEY='False' quotes\n",
    "\n",
    "# when start scrapy shell\n",
    "scrapy shell  --set=\"ROBOTSTXT_OBEY=False\"\n",
    "\n",
    "```\n",
    "\n",
    "1. ignore change conf file: edit settings.py\n",
    "\n",
    "\n",
    "```yaml\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bdeb8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Passing user pamarameters to crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366916a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "```sh\n",
    "scrapy crawl imdb --set=ROBOTSTXT_OBEY='False' -a actor='juliana paes'\n",
    "```\n",
    "\n",
    "\n",
    "```python \n",
    "class ImdbSpider(scrapy.Spider):\n",
    "    name = 'imdb'\n",
    "\n",
    "    start_urls = ['https://secure.imdb.com/ap/signin?openid.pape.max_auth_age=0&openid.return_to=https%3A%2F%2Fwww.imdb.com%2Fregistration%2Fap-signin-handler%2Fimdb_pro_us&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.assoc_handle=imdb_pro_us&openid.mode=checkid_setup&siteState=eyJvcGVuaWQuYXNzb2NfaGFuZGxlIjoiaW1kYl9wcm9fdXMiLCJyZWRpcmVjdFRvIjoiaHR0cHM6Ly9wcm8uaW1kYi5jb20vIn0&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0']\n",
    "\n",
    "    def __init__(self, actor='', **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)  # python3\n",
    "\n",
    "        self.logger.info(f'init spider for actor: {actor}')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5d5e7",
   "metadata": {},
   "source": [
    "## Dealing with cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8b8d7",
   "metadata": {},
   "source": [
    "* Example without scrapy\n",
    "    * can be useful for debug while working with scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63184130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie is set to:\n",
      "{'loggedin': '1', 'username': 'Ryan'}\n",
      "-----------\n",
      "Going to profile page...\n",
      "Hey Ryan! Looks like you're still logged into the site!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "params = {'username': 'Ryan', 'password': 'password'}\n",
    "r = requests.post(\"http://pythonscraping.com/pages/cookies/welcome.php\", params)\n",
    "\n",
    "print(\"Cookie is set to:\")\n",
    "print(r.cookies.get_dict())\n",
    "print(\"-----------\")\n",
    "print(\"Going to profile page...\")\n",
    "\n",
    "r = requests.get(\"http://pythonscraping.com/pages/cookies/profile.php\",\n",
    "cookies=r.cookies)\n",
    "print(r.text)  # <== add this to a selector and extract using scrapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa2cf9",
   "metadata": {},
   "source": [
    "* Some website chnage cookies all the time. You can use the seesion objects to manage the cookies for you\n",
    "    * can be useful for debug while working with scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e1a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie is set to:\n",
      "{'loggedin': '1', 'username': 'username'}\n",
      "-----------\n",
      "Going to profile page...\n",
      "Hey username! Looks like you're still logged into the site!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "session = requests.Session()\n",
    "params = {'username': 'username', 'password': 'password'}\n",
    "s = session.post(\"http://pythonscraping.com/pages/cookies/welcome.php\", params)\n",
    "\n",
    "print(\"Cookie is set to:\")\n",
    "print(s.cookies.get_dict())\n",
    "print(\"-----------\")\n",
    "print(\"Going to profile page...\")\n",
    "s = session.get(\"http://pythonscraping.com/pages/cookies/profile.php\")\n",
    "print(s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ec63cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce119a6c",
   "metadata": {},
   "source": [
    "* Using scrapy\n",
    "    * If you need to to call Request, then it is your responsability to manage cookies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fda931",
   "metadata": {},
   "source": [
    "You need to make sure the user agent is recetly one. Old user agents do not support multiple cookies (modern way to do it)\n",
    "    \n",
    "```python\n",
    "\n",
    " def __init__(self, actor_page='', **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)  # python3\n",
    "\n",
    "        self._credentials = {\n",
    "            'email': 'email'\n",
    "            'pwd': '123'\n",
    "        }\n",
    "\n",
    "        self.header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0'}  #Setting up browser user agent\n",
    "        #self.header =  {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.164 Safari/537.36'}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885cf14",
   "metadata": {},
   "source": [
    "* For page swith login it you have to mange the cookies by yourself when doing the requests\n",
    "\n",
    "\n",
    "```python\n",
    "  def start_requests(self):\n",
    "        \n",
    "        # https://docs.scrapy.org/en/latest/_modules/scrapy/http/request.html\n",
    "        request = Request(\n",
    "            url=self.start_urls[0],\n",
    "            headers=self.header,\n",
    "            meta={'cookiejar':1},       #Open Cookies record and pass Cookies to callback function\n",
    "            callback=self.parse\n",
    "        ) \n",
    "        \n",
    "        cookies = request.cookies  # inspect cookies sent in the request\n",
    "        \n",
    "        return [request] \n",
    "```\n",
    "\n",
    "Any request you must do it:\n",
    "\n",
    "\n",
    "```python \n",
    "# example:\n",
    "request = response.follow(login_url, self.process_login_page, \n",
    "                            meta = {'cookiejar' : response.meta['cookiejar']},\n",
    "                            headers = self.header)\n",
    "\n",
    "# another example\n",
    "return FormRequest.from_response(response,\n",
    "            formdata={\n",
    "            'appActionToken': token, \n",
    "            'email': self._credentials['email'],\n",
    "            'password': self._credentials['pwd']},\n",
    "            meta={'cookiejar':response.meta['cookiejar']},\n",
    "            dont_filter=True,\n",
    "            headers=self.header\n",
    "            , callback=self.after_login)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3769a58",
   "metadata": {},
   "source": [
    "* get the receive cookies\n",
    "\n",
    "```python\n",
    "    cookie = response.headers.getlist('Set-Cookie')\n",
    "    self.logger.info(f\"cookies: {cookie}\") \n",
    "```\n",
    "\n",
    "\n",
    "* print sent cookies\n",
    "\n",
    "\n",
    "```python\n",
    "self.logger.info(\"======================  Cookie Jars\") \n",
    "cookieJar = response.meta.setdefault('cookie_jar', CookieJar())\n",
    "cookieJar.extract_cookies(response, response.request)\n",
    "self.logger.info(f\"cookie_jar: {cookieJar._cookies}\")\n",
    "\n",
    "\n",
    "# from the request\n",
    "\n",
    "cl = request.headers.getlist('Cookie')\n",
    "if cl:\n",
    "            \n",
    "    self.logger.info(f\"LHOF=> Sending cookies to: {request}\")\n",
    "\n",
    "    for c in cl:\n",
    "\n",
    "        self.logger.info(f\"Cookies: {c}\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2fde8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Debbug and develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec471fb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Use selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "865a8bd0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n                ',\n",
       " ' Talent Agent',\n",
       " '\\n                ',\n",
       " '\\n                ',\n",
       " '+1 310 288 8000',\n",
       " ' phone',\n",
       " '\\n                ',\n",
       " '\\n            ']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract current text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' phone']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract parent text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['+1 310 288 8000', '\\n                ']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = Selector(text = \"\"\"\n",
    "            <div id=\"contacts\" class=\"a-section\">\n",
    "                <div class=\"a-column a-span12 a-span-last\"><span class=\"a-size-base a-text-bold\"> Talent Agent</span></div>\n",
    "                <span class=\"a-list-item\"></span>\n",
    "                <span class=\"a-list-item\">+1 310 288 8000<span class=\"a-color-secondary\"> phone</span>\n",
    "                </span>\n",
    "            </div>\n",
    "          \"\"\")\n",
    "\n",
    "divs = val.xpath(\"//div[@id='contacts']\")\n",
    "spans = divs.xpath(\"//span[@class='a-color-secondary']\")\n",
    "\n",
    "print(\"extract all text\")\n",
    "spans.xpath(\"//text()\").getall()\n",
    "\n",
    "print(\"extract current text\")\n",
    "spans.xpath(\"./text()\").getall()\n",
    "\n",
    "print(\"extract parent text\")\n",
    "spans.xpath(\"//span[@class='a-color-secondary']/../text()\").getall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc4aa2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Open requested page on Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b24225d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scrapy.utils.response import response_status_message, open_in_browser\n",
    "\n",
    "\n",
    "def parse_something(self, response):\n",
    "    \n",
    "    open_in_browser(response)\n",
    "    self.logger.error(response.body)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc08d7d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.8566131591797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

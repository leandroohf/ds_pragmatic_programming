{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark (WIP)\n",
    "\n",
    "\n",
    "**TODO: Merging with ds_pragmatic_programming_pyspark**\n",
    "\n",
    "To run this notebook you need to run jupyetr from inside the docker container. Soes not work in binder right now\n",
    "\n",
    "* how to run using docker image for spark\n",
    "\n",
    "```sh\n",
    "\n",
    "# Install images\n",
    "docker pull ucsddse230/cse255-dse230\n",
    "\n",
    "# Run container\n",
    "docker run --name ds_pragmatic -it -p 8890:8888 -v /media/leandroohf/sdb1/leandro/ds_pragmatic_programming:/home/ucsddse230/ ucsddse230/cse255-dse230 /bin/bash\n",
    "\n",
    "# If you need to ssh to the container\n",
    "docker exec -it ds_pragmatic /bin/bash\n",
    "\n",
    "# Run jupyter inside container\n",
    "jupyter notebook\n",
    "\n",
    "```\n",
    "\n",
    "1. http://localhost:8889/tree\n",
    "2. Copy and paste token to login in the notebook\n",
    "\n",
    " \n",
    "* refs:\n",
    "\n",
    "    * https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/course/\n",
    "    * https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/courseware/d1f293d0cb53466dbb5c0cd81f55b45b/fe9a95cc542d4c30b855e632663c4797/8?activate_block_id=block-v1%3ABerkeleyX%2BCS105x%2B1T2016%2Btype%40vertical%2Bblock%4083ff2d3b4e93489b9b7b4861811e0872\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T20:31:46.676642Z",
     "start_time": "2019-10-21T20:31:45.640583Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a54074ae2d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "%pylab inline\n",
    "\n",
    "#start the SparkContext\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(master=\"local[4]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T20:30:20.515980Z",
     "start_time": "2019-10-21T20:30:20.285848Z"
    }
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls data/Weather/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# example of reading form HDFS\n",
    "# supported formats: TEXT, CSV, Avro, Parquet and JSON \n",
    "rddFromFile = sc.textFile(\"hdfs://nn1home:8020/text01.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T20:31:18.769016Z",
     "start_time": "2019-10-21T20:31:18.230659Z"
    }
   },
   "outputs": [],
   "source": [
    "# form txt file \n",
    "!head data/Moby-Dick.txt\n",
    "\n",
    "# method is used to read a text file from HDFS, S3 and any Hadoop supported file system\n",
    "text_file = sc.textFile('data/Moby-Dick.txt')\n",
    "type(text_file)\n",
    "\n",
    "\n",
    "pair_rdd = sc.parallelize([(1,2), (3,4)])\n",
    "print(pair_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T00:43:30.937909Z",
     "start_time": "2019-10-22T00:43:30.698752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leandroohf/Documents/leandro/ds_pragmatic_programming\n",
      "NY.parquet  NY.tgz\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "# data\n",
    "!ls data/Weather/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "* You cannot use any operation on the map function. The operation should NOT depend of the other like subtraction or division. Will get different results while runnning multiple times\n",
    "\n",
    " Transformations on (key,value) rdds. **RDD $\\to$ RDD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map, filter n sample \n",
    "\n",
    "* **No** communication needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_rdd = sc.parallelize([1, 2, 3, 4, 2, 5, 6])\n",
    "\n",
    "# multiplication does not depend of the order\n",
    "pair_rdd = regular_rdd.map( lambda x: (x, x*x) )\n",
    "print(pair_rdd.collect())\n",
    "\n",
    "print(regular_rdd.filter( lambda x: x > 3 ).collect())\n",
    "\n",
    "# sample(withReplacement, fraction, seed)\n",
    "print(regular_rdd.sample(True, 0.5, 11))\n",
    "\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "\n",
    "# LHOF Notes\n",
    "x = 3\n",
    "print('list: ', list(range(x,x+2)))\n",
    "\n",
    "# the lambda function generates for each number i, an iterator that produces i,i+1\n",
    "print(\"After transformation : \", rdd.flatMapValues(lambda x: list(range(x,x+2))).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupbyKey,  reduceByKey n sortByKey\n",
    "\n",
    "**Shuffles:** RDD $\\to$ RDD, **shuffle** needed\n",
    "\n",
    "**Shuffles are costly transfromations**\n",
    "\n",
    "* **Examples:** sort, distinct, repartition, sortByKey, reduceByKey, join [More](http://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations)\n",
    "  * **A LOT** of communication might be needed.\n",
    "\n",
    "\n",
    "**Properties of reduce operations**\n",
    "\n",
    "* Reduce operations **must not depend on the order**\n",
    "  * Order of operands should not matter\n",
    "  * Order of application of reduce operator should not matter\n",
    "\n",
    "* Multiplication and summation are good:\n",
    "\n",
    "```\n",
    "                1 + 3 + 5 + 2                      5 + 3 + 1 + 2 \n",
    "```\n",
    "\n",
    " * Division and subtraction are bad:\n",
    "\n",
    "```\n",
    "                    1 - 3 - 5 - 2                      1 - 3 - 5 - 2\n",
    "```\n",
    "\n",
    "\n",
    "**groupByKey():**\n",
    "Returns a new RDD of `(key,<iterator>)` pairs where the iterator iterates over the values associated with the key.\n",
    "\n",
    "\n",
    "[Iterators](http://anandology.com/python-practice-book/iterators.html) are python objects that generate a sequence of values. Writing a loop over `n` elements as \n",
    "```python\n",
    "for i in range(n):\n",
    "    ##do something\n",
    "```\n",
    "is inefficient because it first allocates a list of `n` elements and then iterates over it.\n",
    "Using the iterator `xrange(n)` achieves the same result without materializing the list. Instead, elements are generated on the fly.\n",
    "\n",
    "To materialize the list of values returned by an iterator we will use the list comprehension command:\n",
    "```python\n",
    "[a for a in <iterator>]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey return (key, <iterator>)\n",
    "\n",
    "A = sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])\n",
    "A.groupByKey().mapValues(lambda x: [elem for elem in x ])\n",
    "\n",
    "# output\n",
    "#[ (1, [3,-5]), (3, [100, 2]) ]\n",
    "\n",
    "print(A.groupByKey().map(lambda elem: (elem[0],[x for x in elem[1] ])).collect())\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "\n",
    "# output\n",
    "# [(1,2),(2,10)]\n",
    "print(\"After transformation : \", rdd.reduceByKey(lambda a,b: a+b).collect())\n",
    "\n",
    "\n",
    "rdd = sc.parallelize([(2,2), (1,4), (3,6)])\n",
    "print(\"Original RDD :\", rdd.collect())\n",
    "print(\"After transformation : \", rdd.sortByKey().collect())\n",
    "\n",
    "# Using sortBy\n",
    "print(\"After transformation : \", rdd.sortBy(lambda x: x[1],ascending=False).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations 2 rdds\n",
    "\n",
    "**subtractByKey**\n",
    "Remove from RDD1 all elements whose key is present in RDD2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LHOF Notes\n",
    "\n",
    "rdd1 = sc.parallelize([(1,2), (2,1), (2,2)])\n",
    "rdd2 = sc.parallelize([(2,5), (3,1)])\n",
    "\n",
    "print('rdd1: ', rdd1.collect())\n",
    "print('rdd2: ', rdd2.collect())\n",
    "print('subtractByKey: ', rdd1.subtractByKey(rdd2).collect())\n",
    "\n",
    "print()\n",
    "# Pay attention. This is a set operation\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "\n",
    "print('x: ', x.collect())\n",
    "print('y: ', y.collect())\n",
    "print('subtract: ', sorted(x.subtract(y).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**join**\n",
    "\n",
    "* A fundamental operation in relational databases.\n",
    "* assumes two tables have a **key** column in common. \n",
    "* merges rows with the same key.\n",
    "\n",
    "\n",
    "When `Join` is called on datasets of type `(Key, V)` and `(Key, W)`, it  returns a dataset of `(Key, (V, W))` pairs with all pairs of elements for each key. Joining the 2 datasets above yields: \n",
    "\n",
    "\n",
    "There are four variants of `join` which differ in how they treat keys that appear in one dataset but not the other.\n",
    "* `join` is an *inner* join which means that keys that appear only in one dataset are eliminated.\n",
    "* `leftOuterJoin` keeps all keys from the left dataset even if they don't appear in the right dataset. The result of leftOuterJoin in our example will contain the keys `John, Jill, Kate`\n",
    "* `rightOuterJoin` keeps all keys from the right dataset even if they don't appear in the left dataset. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John`\n",
    "* `FullOuterJoin` keeps all keys from both datasets. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John, Kate`\n",
    "\n",
    "In outer joins, if the element appears only in one dataset, the element in `(K,(V,W))` that does not appear in the dataset is represented bye `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OuterJoin\n",
    "print('rdd1=',rdd1.collect())\n",
    "print('rdd2=',rdd2.collect())\n",
    "print(\"Result:\", rdd1.rightOuterJoin(rdd2).collect())\n",
    "\n",
    "print()\n",
    "\n",
    "# leftOuterJoin\n",
    "print('rdd1=',rdd1.collect())\n",
    "print('rdd2=',rdd2.collect())\n",
    "print(\"Result:\", rdd1.leftOuterJoin(rdd2).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "Actions on (key,val) RDDs. **RDD $\\to$ Python-object in head node.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  countByKey: returns dictionary\n",
    "A = sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])\n",
    "\n",
    "A.countByKey()\n",
    "\n",
    "# output (dictionnary\n",
    "# {1:2, 3:2}\n",
    "\n",
    "# lookup (key): returns the list of all of the values associated with key\n",
    "A = sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])\n",
    "\n",
    "A.lookup(3)\n",
    "\n",
    "# output (list)\n",
    "# [100,2]\n",
    "\n",
    "#  collectAsMap(): like collect() - collect returns list of tuples -  but returns a map = Dictionary\n",
    "A = sc.parallelize([(1,3), (3,100),(1,-5),(3,2)])\n",
    "A.collectAsMap()\n",
    "\n",
    "# output Dictionary\n",
    "# {1:[3,-5], 3: [100,2]}\n",
    "\n",
    "regular_rdd = sc.parallelize([1, 2, 3, 4, 2, 5, 6])\n",
    "\n",
    "# takeSample(withReplacement, num, [seed])\n",
    "print(regular_rdd.takeSample(True, 5, 11))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Famous word count example (hello word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Famous word count example (hello word)\n",
    "\n",
    "words = ['this', 'is', 'the', 'best', 'mac', 'ever']\n",
    "\n",
    "wordRDD = sc.parallelize(words)\n",
    "\n",
    "# TODO: Review this. Not sure about this code !?\n",
    "wordRDD.reduce(lambda w,v: w if len(w) < len(v) else v)\n",
    "\n",
    "# another example is sum (the prder does not matter)\n",
    "B=sc.parallelize([1,3,5,2])\n",
    "\n",
    "B.reduce(lambda x,y: x+y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe \n",
    "\n",
    "Dataframe are just RDDs with shema (metinfo). Because of the restrictions imposed by the schema, they can be more optmized\n",
    "\n",
    "\n",
    "sheet cheat:\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "# #from pyspark import sqlContext\n",
    "\n",
    "\n",
    "## Option 1 \n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "rdd = sc.parallelize([Row(name=u\"John\",  age=19),\n",
    "                      Row(name=u\"Smith\", age=23),\n",
    "                      Row(name=u\"Sarah\", age=18)])\n",
    "\n",
    "rdd.collect()\n",
    "# Output:\n",
    "# [Row(name=u\"John\",  age=19),\n",
    "#  Row(name=u\"Smith\", age=23),\n",
    "#  Row(name=u\"Sarah\", age=18)]\n",
    "\n",
    "df = sqlContext.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# OPtion 2\n",
    "rdd2 = sc.parallelize([(\"John\",19),(\"Smith\",23),(\"Sarah\",18)])\n",
    "\n",
    "\n",
    "schema = StructType([StructField(\"person_name\", StringType(),False),\n",
    "                     StructField(\"person_age\",  IntegerType(),False)\n",
    "])\n",
    "\n",
    "df2 = sqlContext.createDataFrame(rdd2,schema)\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and write from disk\n",
    "\n",
    "\n",
    "* parquet files (it is folder) are very popular e efficeent for IO in disk\n",
    "* parqet can be query directly from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read\n",
    "parquet_file = 'data/users.parquet'\n",
    "print(parquet_file)\n",
    "\n",
    "df = sqlContext.read.load(parquet_file)\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Write\n",
    "!rm -rv data/df.parquet\n",
    "\n",
    "# This throw an error if file already exit\n",
    "df.write.save(\"df.parquet\")\n",
    "!ls \n",
    "!echo\n",
    "!ls df.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query \n",
    "\n",
    "\n",
    "* parquet file\n",
    "* Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this way because the parquet file was not distributed in the hadoop cluster you do not have parallelism\n",
    "parquet_file = \"data/Weather/NY.parquet\"\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT station,measurement,year \n",
    "FROM parquet.`%s` \n",
    "WHERE measurement=\\\"SNOW\\\" \n",
    "\"\"\"%(parquet_file)\n",
    "\n",
    "print(query)\n",
    "\n",
    "df2 = sqlContext.sql(query)\n",
    "print(df2.count(),df2.columns)\n",
    "\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/people.json\"\n",
    "\n",
    "# Create a DataFrame from the file(s) pointed to by path\n",
    "people = sqlContext.read.json(path)\n",
    "#print('people is a',type(people))\n",
    "\n",
    "print(people.count())\n",
    "\n",
    "# The inferred schema can be visualized using the printSchema() method.\n",
    "people.show()\n",
    "\n",
    "\n",
    "## Needs to register the dataframe first\n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "for each in teenagers.collect():\n",
    "    print(each[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\",\"favorite_color\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('measurement').agg({'year': 'min', 'station':'count'}).show(5)\n",
    "\n",
    "print(\"True value: {}\".format(df.select('station').distinct().count()))\n",
    "df.agg({'station':'approx_count_distinct'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Stat Functions\n",
    "\n",
    "Methods for statistics functionality. [documented here](http://takwatanabe.me/pyspark/generated/generated/pyspark.sql.DataFrameStatFunctions.html)\n",
    "\n",
    "**Some of these functions use inference to get aproximated estimations** and it is a way more faster if the dataframe or RDD are huge (like in Big Data)\n",
    "\n",
    "* **approxQuantile(col, probabilities, relativeError)**\tCalculates the approximate quantiles of a numerical column of a DataFrame.\n",
    "* **corr(col1, col2[, method])**\tCalculates the correlation of two columns of a DataFrame as a double value.\n",
    "* **cov(col1, col2)**\tCalculate the sample covariance for the given columns, specified by their names, as a double value.\n",
    "* **crosstab(col1, col2)**\tComputes a pair-wise frequency table of the given columns.\n",
    "* **freqItems(cols[, support])**\tFinding frequent items for columns, possibly with false positives.\n",
    "* **sampleBy(col, fractions[, seed])**\tReturns a stratified sample without replacement based on the fraction given on each stratum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This make the quantiles\n",
    "print([0.1*i for i in range(1,10)])\n",
    "\n",
    "\n",
    "# That is very cool. Based on inference**\n",
    "print()\n",
    "print(\"Get the qiantiles\")\n",
    "print('with accuracy 0.1: ',df.approxQuantile('year', [0.1*i for i in range(1,10)], 0.1))\n",
    "print('with accuracy 0.01: ',df.approxQuantile('year', [0.1*i for i in range(1,10)], 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes UDF\n",
    "\n",
    "    **Similar to lambda function**\n",
    "   \n",
    "    User define functions *UDF* are functions that can be ran against a\n",
    "    dataframe and spark can optimize it\n",
    "\n",
    "    Steps:\n",
    "    1. define function that operates in the value field\n",
    "    2. register the function\n",
    "    3. use the function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def packArray(a):\n",
    "    \"\"\"\n",
    "    pack a numpy array into a bytearray that can be stored as a single \n",
    "    field in a spark DataFrame\n",
    "\n",
    "    :param a: a numpy ndarray \n",
    "    :returns: a bytearray\n",
    "    :rtype:\n",
    "\n",
    "    \"\"\"\n",
    "    if type(a)!=np.ndarray:\n",
    "        raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "    return bytearray(a.tobytes())\n",
    "\n",
    "def unpackArray(x,data_type=np.float16):\n",
    "    \"\"\"\n",
    "    unpack a bytearray into a numpy.ndarray\n",
    "\n",
    "    :param x: a bytearray\n",
    "    :param data_type: The dtype of the array. This is important because if determines how many bytes go into each entry in the array.\n",
    "    :returns: a numpy array\n",
    "    :rtype: a numpy ndarray of dtype data_type.\n",
    "\n",
    "    \"\"\"\n",
    "    return np.frombuffer(x,dtype=data_type)\n",
    "\n",
    "def Count_nan(V):\n",
    "\n",
    "    A=unpackArray(V,data_type=np.float16)\n",
    "\n",
    "    return int(sum(np.isnan(A)))\n",
    "\n",
    "#register\n",
    "Count_nan_udf = udf(Count_nan, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"na_no\", Count_nan_udf(df.Values))\n",
    "\n",
    "df.select(\"Station\",\"Year\",\"Measurement\",\"na_no\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
